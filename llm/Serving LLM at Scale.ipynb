{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c45fd35b-85be-463e-bcd7-7f74c7c064d8",
   "metadata": {},
   "source": [
    "# Serving Large Language Models (LLMs) at Scale on AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3971cc67-4580-4a79-a6d2-8daaa8291951",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "Language Model (LLM) serving at scale refers to the process of delivering an LLM as a service to a large number of users efficiently. This involves setting up a robust and efficient infrastructure to handle a large number of requests quickly and at a low latency.\n",
    "\n",
    "LLM inference at scale can be achieved through various techniques such as model parallelism, data parallelism, efficient data handling, optimized inference, asynchronous inference, auto-scaling, load balancing, optimized hardware, caching, and using model serving frameworks. The goal is to minimize the latency of individual requests, reduce the computational complexity of the model, and improve the overall system throughput to meet the demands of a larger number of users. \n",
    "\n",
    "By serving an LLM at scale, you can make it accessible to a wider audience and enable them to use the model to generate text, translate languages, answer questions, and perform other natural language processing tasks quickly and efficiently.\n",
    "\n",
    "In this article, I will use Amazon SageMaker to show how we can control resources to serve our LLM models at scale. It may include the number of GPUs, memory, or the replicas assigned to serve dynamic amounts of requests to our LLM models. Moreover, I also show how to attach an auto-scaling policy to our serving endpoint which will scale our endpoint automatically when workload varies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37be1ebb-d9a3-42f5-adf2-689994cf4eb3",
   "metadata": {},
   "source": [
    "## AWS Deep Learning Containers\n",
    "\n",
    "\n",
    "Large Language Models (LLMs) have become a forefront of innovation in artificial intelligence, capturing the attention of academic establishments, tech companies and enthusiasts with their sophisticated capabilities. Models built on architectures like GPT and Llama have rapidly gained traction for a wide range of uses such as language comprehension, conversational interfaces, and automated content creation. This surge in demand has led many companies to explore and integrate LLM-driven features into their products.\n",
    "\n",
    "However, deploying LLMs on a large scale involves complex engineering challenges. To ensure a seamless user experience, hosting services for LLMs need to maintain quick response times while supporting numerous users simultaneously. Due to the substantial resource demands of these models, standard inference frameworks often fall short in delivering the necessary optimizations for optimal resource use and performance.\n",
    "\n",
    "Key optimizations that can enhance LLM hosting include:\n",
    "\n",
    "* Tensor parallelism, which spreads computation across multiple processing units.\n",
    "* Model quantization, which reduces the modelâ€™s memory usage.\n",
    "* Dynamic batching of requests to increase processing throughput and more.\n",
    "\n",
    "\n",
    "Recently, AWS has released a new Hugging Face Deep Learning Container (DLC) for inference with Large Language Models (LLMs). This new Hugging Face LLM DLC is powered by Text Generation Inference (TGI), an open source, purpose-built solution for deploying and serving Large Language Models. TGI enables high-performance text generation using Tensor Parallelism and dynamic batching for the most popular open-source LLMs. The Hugging Face LLM DLC incorporates all the aforementioned optimizations as standard features, simplifying the large-scale deployment of LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1ad4f0-4f1e-405f-b7e9-345d33463d79",
   "metadata": {},
   "source": [
    "# Serving Llama 3 at Scale in SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ab88b9-cc3c-4ed1-88e9-bbfe4115c0f5",
   "metadata": {},
   "source": [
    "Let's start off by installing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e139c7a8-0c79-480a-9d29-e11fd4e418e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U sagemaker transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33078531-ce7c-4476-b7b0-9bf8216b9e32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::609362070692:role/service-role/AmazonSageMaker-ExecutionRole-20231122T115899\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess.boto_region_name\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f68d71a-300a-42c8-be9f-5bd4218acbaa",
   "metadata": {},
   "source": [
    "Next, we need to retrieve the container uri and provide it to our HuggingFaceModel model class with a image_uri pointing to the image. To retrieve the new Hugging Face LLM DLC in Amazon SageMaker, we can use the <code>get_huggingface_llm_image_uri</code> method provided by the sagemaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32e14815-19d4-4e8e-9ee7-fb52127312dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n",
      "llm image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.1-tgi2.0-gpu-py310-cu121-ubuntu22.04\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    ")\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"Your token\")\n",
    "\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee31753a-2dbe-4f58-a0df-244bf624dca3",
   "metadata": {},
   "source": [
    "To deploy Llama 3 70B to Amazon SageMaker we create a <code>HuggingFaceModel</code> model class and define our endpoint configuration including the hf_model_id, instance_type etc. We will use a g5.45xlarge instance type, which has 8 NVIDIA A10G GPUs and 192GB of GPU memory. You need atleast > 100GB of GPU memory to run Mixtral 8x7B in float16 with decent input length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ce25cf3-7903-41f0-852e-1c9291ef5f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    " \n",
    "# sagemaker config\n",
    "instance_type = \"ml.p3.8xlarge\"\n",
    "number_of_gpu = 4\n",
    "health_check_timeout = 300\n",
    " \n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"meta-llama/Meta-Llama-3-8B-Instruct\", \n",
    "  'SM_NUM_GPUS': \"1\", # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': \"2048\",  # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': \"4096\",  # Max length of the generation (including input text)\n",
    "  'MAX_BATCH_TOTAL_TOKENS': \"8192\",  # Limits the number of tokens that can be processed in parallel during the generation. The context window of llama3 models is 8192 tokens\n",
    "}\n",
    " \n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bf18a3-913a-4baf-b976-277e6695fcae",
   "metadata": {},
   "source": [
    "With <code>ResourceRequirements</code> you can assign endpoint resources to a model. These resources include CPU cores, accelerators, and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b588ee2-1ffa-41ec-badc-a1cdfd3c14d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements\n",
    "\n",
    "llama3_resource_config = ResourceRequirements(\n",
    "    requests = {\n",
    "        \"copies\": 4, # Number of replicas\n",
    "        \"num_accelerators\": 1, # Number of GPUs\n",
    "        \"num_cpus\": 6,  # Number of CPU cores 32 // num_replica - more for management\n",
    "        \"memory\": 40 * 1024,  # Minimum memory (MB) 244 // num_replica - more for management\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cee1277-b9b5-478c-bf17-fa293741491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import uuid\n",
    "from sagemaker.enums import EndpointType\n",
    "\n",
    "llm = llm_model.deploy(\n",
    "    initial_instance_count=1, # number of instances\n",
    "    instance_type=instance_type, # base instance type\n",
    "    resources=llama3_resource_config, # resource config for multi-replica\n",
    "    container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    "    endpoint_name=f\"llama3-chat-{str(uuid.uuid4())}\", # name needs to be unique\n",
    "    endpoint_type=EndpointType.INFERENCE_COMPONENT_BASED, # needed to use resource config\n",
    "    tags=[{\"Key\": \"aKey\", \"Value\": \"aValue\"}],\n",
    "    model_name=\"llama3-chat\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325f2769-8311-4179-9c4d-806dc452a5fe",
   "metadata": {},
   "source": [
    "# AutoScaling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade5fef6-efb1-4587-af81-72eae2fa987c",
   "metadata": {},
   "source": [
    "Amazon SageMaker offers automatic scaling (auto scaling) for hosted models, which dynamically adjusts the number of instances based on workload changes. When the workload increases, auto scaling activates additional instances. Conversely, when the workload decreases, it removes unnecessary instances so that you don't pay for provisioned instances that you aren't using.\n",
    "\n",
    "First, we need to define a scaling policy that adds and removes the number of instances for our production endpoint in response to workload changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af91ed5-a9ce-4145-b489-37dfecea0cc3",
   "metadata": {},
   "source": [
    "# Autoscaling Policies\n",
    "\n",
    "There are three main types of autoscaling policies for SageMaker Endpoints: target tracking, simple, and step scaling:\n",
    "\n",
    "* Target Tracking:\n",
    "With the target tracking scaling policy, you choose an Amazon CloudWatch metric and target value, such as SageMaker VariantInvocationsPerInstance = 100, and SageMaker can keep VariantInvocationsPerInstance at, or close to 100. This approach is very common due to its ease of configuration.\n",
    "\n",
    "* Simple Scaling:\n",
    "The simple scaling policy triggers a scaling event based on a specified metric at a defined threshold with a fixed amount of scaling. For instance, \"when SageMaker VariantInvocationsPerInstance > 1000, add 10 instances.\" This strategy requires more configuration but offers greater control compared to target tracking.\n",
    "\n",
    "* Step Scaling:\n",
    "You can use step scaling when you require an advanced configuration, such as specifying how many instances to deploy under what conditions. For example, \"when SageMaker VariantInvocationsPerInstance > 1000, add 10 instances; when SageMaker VariantInvocationsPerInstance > 2000, add 50 instances.\" This approach demands the most configuration but provides the highest level of control, especially for handling spiky traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd150025-10bf-48a0-8991-2fdc24d6b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoscale = boto3.Session().client(service_name=\"application-autoscaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4505a86-3b4f-4043-abbc-8ab9c60d3c59",
   "metadata": {},
   "source": [
    "First, we need to register the resource as a scalable target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3925aa-9741-4a85-9431-71fc3f2d52bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoscale.register_scalable_target(\n",
    "    ServiceNamespace=\"sagemaker\", #1\n",
    "    ResourceId=\"endpoint/\" + endpoint_name + \"/variant/AllTraffic\", #2\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\", #3\n",
    "    MinCapacity=1, #4\n",
    "    MaxCapacity=2, #5\n",
    "    RoleARN=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c409362-e3e3-4275-96ba-eae6046c80f5",
   "metadata": {},
   "source": [
    "In the above configuration, we have defined #1) The AWS service name, #2) The identifier of the resource that is associated with the scalable target. #3) The scalable property associated with the scalable target (e.g., the number of EC2 instances for a SageMaker model endpoint variant) #4, #5) The minimum/maximum value that you plan to scale in/ scale out to. Please refere to the [documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/application-autoscaling/client/register_scalable_target.html) for other configurable parameters.\n",
    "\n",
    "After registering sagemaker as a scalable target, we can creates or updates the scaling policy for that target using <code>put_scaling_policy</code>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1080813-5d22-4e7b-a8e7-ef9c4d115a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoscale.put_scaling_policy(\n",
    "    PolicyName=\"autoscale-policy-llama3-8b\",\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=\"endpoint/\" + endpoint_name + \"/variant/AllTraffic\",\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    PolicyType=\"TargetTrackingScaling\",\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        'TargetValue': 20,\n",
    "        'PredefinedMetricSpecification': {\n",
    "            'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance'            \n",
    "        }, \n",
    "        'ScaleOutCooldown': 60,\n",
    "        'ScaleInCooldown': 300,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874d5897-9eee-4985-9c3c-5c00e4beb885",
   "metadata": {},
   "source": [
    "Here, we need to define PolicyType which will be one of the auto-scaling policies described ealier. We define Target Value which acts as a metric trigger to scale out or scale in the defined scalable dimension (e.g., sagemaker:variant:DesiredInstanceCount). For example in the above configuration, when the number of sagemaker invocations per instance (SageMakerVariantInvocationsPerInstance) exceeds 100 per minute a new instance is added to our deployed endpoint. \n",
    "\n",
    "A cooldown period defines the time interval the scaling policy waits before initiating another scaling action. This mechanism helps prevent over-scaling.\n",
    "\n",
    "<code>ScaleOutCooldown</code>: Following a successful scale-out by the scaling policy, auto scaler begins calculating the cooldown period. The policy will not increase the desired capacity again unless a more significant scale-out event occurs or the cooldown period finishes.\n",
    "\n",
    "<code>ScaleInCooldown</code>: To ensure application availability, upcoming scale-in activities are suspended until the scale-in cooldown period has ended. Default value is 300 seconds for both of them.\n",
    "\n",
    "Figure 1 shows how auto-scaling works. \n",
    "\n",
    "<center><figure><img src=\"../imgs/scaling.png\" alt=\"drawing\" width=\"800\"/><figcaption>Fig. 1: Auto-scaling Policy</figcaption></figure></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a0df10-fca1-4e62-9a2b-31753d501a19",
   "metadata": {},
   "source": [
    "## Trigger autoscaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf8918d-f928-41ab-8f7e-4f074673b77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config[\"HF_MODEL_ID\"])\n",
    "\n",
    "# Conversational messages\n",
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"You are an helpful Travel Assistant.\"},\n",
    "  {\"role\": \"user\", \"content\": \"Where is a good vacation destination in north america for summer?\"},\n",
    "]\n",
    "\n",
    "# generation parameters\n",
    "parameters = {\n",
    "    \"do_sample\" : True,\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 50,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "    \"return_full_text\": False,\n",
    "}\n",
    "\n",
    "for i in range(0, 100):\n",
    "    res = llm.predict(\n",
    "      {\n",
    "        \"inputs\": tokenizer.apply_chat_template(messages, tokenize=False),\n",
    "        \"parameters\": parameters\n",
    "       })\n",
    "\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d1b115-1c38-47e1-963f-5064243d321b",
   "metadata": {},
   "source": [
    "In the above code snippet, <code>apply_chat_template</code> with convert the message to a chat template of the underlying LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0838ace-b1da-4da0-bd49-8b3e66c27cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoscale.describe_scaling_activities(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=\"endpoint/\" + endpoint_name + \"/variant/AllTraffic\",\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    MaxResults=100\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
