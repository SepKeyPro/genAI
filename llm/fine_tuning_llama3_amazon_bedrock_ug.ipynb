{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/SepKeyPro/genAI/blob/main/fine_tuning_llama3_amazon_bedrock_ug.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZHVa60QG1wB"
   },
   "source": [
    "# Fine-tuning Llama-3-8B on a custom dataset\n",
    "\n",
    "In this notebook, I am going to show you how to fine-tune a Large Language Model (LLM) such as Llama3 on a custom dataset. For this demonstration, I have generated a custom dataset about \"Amazon Bedrock\". I generated a dataset from [Amazon Bedrock Documentation](https://docs.aws.amazon.com/bedrock/), and converted it to a Llam3-8B chat template to be used for model fine-tuning. You can get the dataset from Hugging Face hub [Here](https://huggingface.co/datasets/SepKeyPro/amazon-bedrock-ug-llama3-8B-Instruct-1k?row=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "SvegvDEMbdob"
   },
   "outputs": [],
   "source": [
    "pip install -U transformers datasets accelerate peft bitsandbytes git+https://github.com/huggingface/trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "H2J3LNOhcla2"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "import torch\n",
    "from transformers import(\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "from huggingface_hub import login\n",
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "vf7ZugKze8EZ"
   },
   "outputs": [],
   "source": [
    "login(token=\"Your access token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "GOZ-XyYnekJ8"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"SepKeyPro/amazon-bedrock-ug-llama3-8B-Instruct-1k\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Vl5q_Jp3e4Ja"
   },
   "outputs": [],
   "source": [
    "base_model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "new_model = \"llama-3-8b-amazon-bedrock\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "wM-U6YLfyAqR"
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=base_model_name, tokenizer=tokenizer, max_length=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kw0rmR_XJEz-"
   },
   "source": [
    "Let's ask Llama3 about Amazon Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5eHT6EpPuBlr",
    "outputId": "4d17b346-cfbf-4e70-bff1-03475cb29f98"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant chatbot.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is Amazon Bedrock?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Amazon Bedrock is a cloud-based data warehousing service offered by Amazon Web Services (AWS). It's designed to help organizations manage and analyze large amounts of data from various sources, providing a unified view of their business operations.\n",
      "\n",
      "Amazon Bedrock is built on top of Amazon Redshift, a popular data warehousing solution, and is optimized for petabyte-scale data storage and querying. It provides a scalable and secure platform for storing, processing, and analyzing large datasets, making it suitable for complex analytics workloads.\n",
      "\n",
      "Some key features of Amazon Bedrock include:\n",
      "\n",
      "1. Scalability: Supports petabyte-scale data storage and querying, making it suitable for large-scale analytics workloads.\n",
      "2. Security: Offers robust security features, including encryption, access controls, and auditing, to ensure data protection.\n",
      "3. Performance: Optimized for fast query performance, with support for complex queries and data analysis.\n",
      "4. Integration: Integrates with other AWS services, such as Amazon S3, Amazon EMR, and Amazon SageMaker, to provide a comprehensive analytics platform.\n",
      "5. Cost-effective: Provides a cost-effective solution for data warehousing and analytics, with pricing based on the amount of data stored and queried.\n",
      "\n",
      "Amazon Bedrock is particularly useful for organizations that need to analyze large amounts of data from various sources, such as customer behavior, sales data, or IoT sensor\n"
     ]
    }
   ],
   "source": [
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is Amazon Bedrock?\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(chat, add_generation_prompt=True, tokenize=False)\n",
    "out = pipe(prompt)\n",
    "generated_text = out[0]['generated_text'].strip()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rj9HKz2vJMs_"
   },
   "source": [
    "As you can see above, Llama3 is hullucinating about Amazon Bedrock. As you may know, Amazon Bedrock is a fully-managed service for genAI application development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EtfGqNYpgJwc"
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "t1mYOxdzgwdi"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "M1UMbo3BoBpF"
   },
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    max_steps=-1,\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=10,\n",
    "    output_dir=\"./results\",\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    warmup_steps=100,\n",
    "    bf16=True,\n",
    "    # report_to=\"wandb\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRlwkh5bJ651"
   },
   "source": [
    "For fine-tuning purpose, I have used A100 GPU with 40 GB of RAM on Colab with 11.77$/hr usage rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 917
    },
    "id": "4Q4z_WuBraLp",
    "outputId": "01090f6f-53ab-4fe7-dffb-82eef22bb7e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='247' max='247' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [247/247 10:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.741400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.648300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.443100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.216900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.121800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.971800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.941700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.770100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.660400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.711300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.462400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.625700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.705100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.618600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.469200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.539700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.588600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.516500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.539600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.632300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.544200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.537500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.471200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.545800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=247, training_loss=1.823735391562767, metrics={'train_runtime': 615.686, 'train_samples_per_second': 1.605, 'train_steps_per_second': 0.401, 'total_flos': 2372689927446528.0, 'train_loss': 1.823735391562767, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_A4AcJYGwY8"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwrgE-DEGxKn"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bH5Xpqh1zdmO",
    "outputId": "b9b272cf-6ec1-4c6f-8473-62c0d5d7b4b0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "358d143e7e154e7885262db991788d09",
      "74e58ec1202740e1887a3c6631f216d7",
      "eb2313afbcc8464499b4c72c8aa6a690",
      "d740c8dcb759419991e5fa12417ba4f9",
      "769cabdf00d64ce78b28d362447490c7",
      "0f3af769f98a4b73975a85d6a1397a55",
      "a0b6967b6db74a69b09869664b4f9c08",
      "083dc63df0da4b63824f42d28b69e155",
      "99e3d0b2b1c0424fac7c64a86f2fa4c5",
      "31fef35fcdda489cb14b4bf06fb77090",
      "c2a6c286351946ab98cecaf203b2a9e2"
     ]
    },
    "id": "ouj-Zhx7hiHi",
    "outputId": "d5964992-4dc5-4d3a-f8e4-a0c712f25ecb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358d143e7e154e7885262db991788d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "mSk7-7buu0ay"
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsTG2h2XLm3L"
   },
   "source": [
    "Let's ask the question again, But this time from the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NwmaSRsBx9Wn",
    "outputId": "adeed4e0-8b1b-40ff-ce31-b87b246ed521"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant chatbot.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is Amazon Bedrock?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Amazon Bedrock is a suite of machine learning tools that makes it easy to build and train custom models using your own data. It provides a range of tools and APIs to help you build and deploy models, and also offers a range of pre-trained models that you can use in your applications.\n"
     ]
    }
   ],
   "source": [
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is Amazon Bedrock?\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(chat, add_generation_prompt=True, tokenize=False)\n",
    "out = pipe(prompt)\n",
    "generated_text = out[0]['generated_text'].strip()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZ88mTB2LxiN"
   },
   "source": [
    "As you can see, this time the answer is acceptable! As we know Amazon Bedrock is a platform hosting many pre-trained model in order to develop and deploy genAI applications."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP/f4u4KJ0m8zAVqnUD58I8",
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "083dc63df0da4b63824f42d28b69e155": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f3af769f98a4b73975a85d6a1397a55": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31fef35fcdda489cb14b4bf06fb77090": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "358d143e7e154e7885262db991788d09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_74e58ec1202740e1887a3c6631f216d7",
       "IPY_MODEL_eb2313afbcc8464499b4c72c8aa6a690",
       "IPY_MODEL_d740c8dcb759419991e5fa12417ba4f9"
      ],
      "layout": "IPY_MODEL_769cabdf00d64ce78b28d362447490c7"
     }
    },
    "74e58ec1202740e1887a3c6631f216d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f3af769f98a4b73975a85d6a1397a55",
      "placeholder": "​",
      "style": "IPY_MODEL_a0b6967b6db74a69b09869664b4f9c08",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "769cabdf00d64ce78b28d362447490c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99e3d0b2b1c0424fac7c64a86f2fa4c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a0b6967b6db74a69b09869664b4f9c08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c2a6c286351946ab98cecaf203b2a9e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d740c8dcb759419991e5fa12417ba4f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31fef35fcdda489cb14b4bf06fb77090",
      "placeholder": "​",
      "style": "IPY_MODEL_c2a6c286351946ab98cecaf203b2a9e2",
      "value": " 4/4 [00:09&lt;00:00,  2.07s/it]"
     }
    },
    "eb2313afbcc8464499b4c72c8aa6a690": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_083dc63df0da4b63824f42d28b69e155",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_99e3d0b2b1c0424fac7c64a86f2fa4c5",
      "value": 4
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
