{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a094959-cfbb-4c97-aa12-8cf6e87f1bbf",
   "metadata": {},
   "source": [
    "# Resume Specialist Using Self-Refine Technique\n",
    "\n",
    "You want to write a resume, but you don't like writing like me! Or you are not in the mood. Are you looking for a resume specialist who writes a resume for you while you talk about your skills, education, experience, etc. If so, please read this post on how to create a resume specialist that generates and criticizes itself until it comes up with the most optimal resume for the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98676e1-82ca-4c5c-b261-4e892dd175e6",
   "metadata": {},
   "source": [
    "This resume specialist uses a self reflection technique to criticize itself. For that purpose, I used LangGraph. You can read more about LangGraph here. In self reflection technique, LLM observes its past actions and evaluates them in order to improve the quality of output later on. \\\n",
    "Here are the overview of the use case:\n",
    "* Generate the transcription of the audio resume using a speech-to-text model.\n",
    "* Generate the first draft of the resume.\n",
    "* Criticize the generated resume.\n",
    "* Continue until the stop condition is satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567ee1d5-309e-4ac8-a4ec-bc9ba3d1225c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><figure><img src=\"imgs/resume_specialist diagram.jpg\" alt=\"drawing\" width=\"1000\"/><figcaption>Fig. 1: Resume specialist architecture</figcaption></figure></center>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "328a25b6-0bbd-4998-b433-e547b0acb029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.4 which is incompatible.\n",
      "jupyterlab 3.4.4 requires jupyter-server~=1.16, but you have jupyter-server 2.12.1 which is incompatible.\n",
      "jupyterlab-server 2.10.3 requires jupyter-server~=1.4, but you have jupyter-server 2.12.1 which is incompatible.\n",
      "notebook 6.5.6 requires jupyter-client<8,>=5.3.4, but you have jupyter-client 8.6.0 which is incompatible.\n",
      "notebook 6.5.6 requires pyzmq<25,>=17, but you have pyzmq 25.1.2 which is incompatible.\n",
      "panel 0.13.1 requires bokeh<2.5.0,>=2.4.0, but you have bokeh 3.3.2 which is incompatible.\n",
      "sagemaker 2.199.0 requires urllib3<1.27, but you have urllib3 2.1.0 which is incompatible.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\n",
      "spyder 5.3.3 requires ipython<8.0.0,>=7.31.1, but you have ipython 8.18.1 which is incompatible.\n",
      "spyder 5.3.3 requires pylint<3.0,>=2.5.0, but you have pylint 3.0.2 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires ipython<8,>=7.31.1; python_version >= \"3\", but you have ipython 8.18.1 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires jupyter-client<8,>=7.3.4; python_version >= \"3\", but you have jupyter-client 8.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -U --quiet  langchain langgraph langchain-fireworks openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eb9681a-7887-4eeb-ad82-c0912a6fbe89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your FIREWORKS API key:  ········\n",
      "Enter your OpenAI API Key:  ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "os.environ['FIREWORKS_API_KEY'] = getpass('Enter your FIREWORKS API key: ')\n",
    "os.environ['OPENAI_API_KEY'] = getpass('Enter your OpenAI API Key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "666f50a9-6cf0-47a4-9a18-c9846ea30f4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "from typing import List, Sequence\n",
    "from langchain_fireworks import ChatFireworks\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, BaseMessage\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from typing import List, Sequence\n",
    "from langgraph.graph import END, MessageGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b6eb4c-4416-41e9-a149-e0a3765c822b",
   "metadata": {},
   "source": [
    "As an LLM model, I choose Mixtral 8x7B instruction model (aka chat model). Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mistral-8x7B outperforms Llama 2 70B on most benchmarks with 6x faster inference. Read more about Mixtral [here](https://mistral.ai/news/mixtral-of-experts/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c046db1d-f227-4c5e-bf5b-98242218e754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatFireworks(\n",
    "    model=\"accounts/fireworks/models/mixtral-8x7b-instruct\", max_tokens = 32768)\n",
    "# llm = ChatMistralAI(model=\"mistral-small-latest\", model_kwargs={\"max_tokens\": 32768})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9be24eb-cc05-4e1d-a649-fe457665b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate(state: Sequence[BaseMessage]):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            \"system\",\n",
    "            \"You are a resume assistant tasked with writing excellent resumes.\"\n",
    "            \" Generate the best resume possible for the user's request.\"\n",
    "            \" If the user provides critique, respond with a revised version of your previous attempts.\",\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm\n",
    "    # print(\"###### message type: #######\",messages[0].type) \n",
    "    return await chain.ainvoke({\"messages\": state})   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad9e9ae6-4102-4b98-8200-a27897f44ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def refine(messages: Sequence[BaseMessage]) -> List[BaseMessage]:\n",
    "    review_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            \"system\",\n",
    "            \"You are a resume reviewer evaluating a resume submission. Generate critique and recommendations for the user's submission.\"\n",
    "            \" Provide detailed recommendations.\",\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    switch_roles = {\"ai\": HumanMessage, \"human\": AIMessage}\n",
    "    transformed_messages = [messages[0]] + [\n",
    "    switch_roles[msg.type](content=msg.content) for msg in messages[1:]]\n",
    "    \n",
    "    chain = review_prompt | llm\n",
    "    refinement = await chain.ainvoke({\"messages\": transformed_messages})\n",
    "    return HumanMessage(refinement.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4648ba7a-4438-43fd-95b6-595592fb8ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def critisize(state: List[BaseMessage]):\n",
    "    print(\"$$$$$$$ messages length: $$$$$$$$$\", len(state))\n",
    "    if len(state) >= 6:\n",
    "        # End after 3 iterations\n",
    "        return END\n",
    "    return \"refine\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1abf4b-d492-44d7-baef-a258c313266d",
   "metadata": {},
   "source": [
    "## We define our graph as follows:\n",
    "* generate node: is responsible for generating a resume based on the resume transcription of the user\n",
    "* reflect node: criticizes the generated resume and gives several recommendations on it\n",
    "* should_continue : is a conditional edge which decides to repeat the process of generat and review or quit the loop and output the final version\n",
    "* reflect - generate: is a normal edge from reflect node to generate node which causes the LLM to revise the previous attemp and apply the new comments to the CV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a071a120-1602-4bf7-9078-b2f8972cb981",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><figure><img src=\"imgs/resume_specialist.jpg\" alt=\"drawing\" width=\"600\"/><figcaption>Fig. 1: Graph of the example</figcaption></figure></center>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db886db2-ef41-4622-805f-c0f9a1b5e33c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph = MessageGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e5e7c45-83f7-4eb0-aa7c-38197c65e055",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph.add_node(\"generate\",generate)\n",
    "graph.add_node(\"refine\",refine)\n",
    "graph.set_entry_point(\"generate\")\n",
    "\n",
    "\n",
    "graph.add_conditional_edges(\"generate\", critisize)\n",
    "graph.add_edge(\"refine\", \"generate\")\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c112076-5d76-495c-b5c2-566d61662866",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Joe Smith. I got my PhD degree in computer science from University of Toronto in 2017. After that, I worked for X company as a machine learning engineer. My task was to develop different machine learning models for X project. I also mentored some junior developers as well. I also deployed and monitored the models in production. After that, I joined Y company in 2022 as a senior language model researcher. My task was to conduct research on LLM models and how to fine tune and also augment them with some techniques. I also prototyped some use cases using the available large language tools. About my skills, I am proficient in Python and I also have a good experience working with deep learning frameworks such as PyTorch and TensorFlow. I also have a good experience using Panda and SQL for data manipulation and I am able to work with machine learning libraries such as Scikit-learn. If you want to reach me, my email is joe.smith at example.com\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "audio_file = open(\"docs/resume.m4a\", \"rb\")\n",
    "resume_transcription = client.audio.transcriptions.create(\n",
    "  model=\"whisper-1\", \n",
    "  file=audio_file, \n",
    "  response_format=\"text\"\n",
    ")\n",
    "print(resume_transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f730de03-2857-4628-9c82-af269bae5308",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$$$$$ messages length: $$$$$$$$$ 2\n",
      "{'generate': AIMessage(content='**Joe Smith**\\nPhD in Computer Science, University of Toronto (2017)\\nEmail: joe.smith@example.com\\n\\n**Professional Summary**\\nExperienced Machine Learning Engineer and Senior Language Model Researcher with a strong background in developing machine learning models and conducting research on large language models. Proficient in Python and deep learning frameworks such as PyTorch and TensorFlow. Skilled in using Panda and SQL for data manipulation and machine learning libraries such as Scikit-learn.\\n\\n**Professional Experience**\\n\\n*Senior Language Model Researcher, Y Company (2022-Present)*\\n\\n* Conduct research on large language models (LLM) and their fine-tuning techniques\\n* Prototype use cases using available large language tools\\n\\n*Machine Learning Engineer, X Company (2017-2022)*\\n\\n* Developed various machine learning models for X project\\n* Mentored junior developers\\n* Deployed and monitored models in production\\n\\n**Education**\\n\\n*PhD in Computer Science, University of Toronto (2017)*\\n\\n*Master of Science in Computer Science, University of Toronto (2014)*\\n\\n*Bachelor of Science in Computer Science, University of Toronto (2012)*\\n\\n**Skills**\\n\\n* Programming Languages: Python\\n* Deep Learning Frameworks: PyTorch, TensorFlow\\n* Data Manipulation: Panda, SQL\\n* Machine Learning Libraries: Scikit-learn\\n\\nPlease let me know if there are any changes or additions you would like me to make to this resume. I am happy to revise it based on your feedback.\\n\\nRevised version:\\n\\n**Joe Smith**\\nPhD in Computer Science, University of Toronto (2017)\\nEmail: joe.smith@example.com\\n\\n**Professional Summary**\\nExperienced Machine Learning Engineer and Senior Language Model Researcher with a strong background in developing machine learning models, conducting research on large language models, and mentoring junior developers. Proficient in Python and deep learning frameworks such as PyTorch and TensorFlow. Skilled in using Panda and SQL for data manipulation and machine learning libraries such as Scikit-learn.\\n\\n**Professional Experience**\\n\\n*Senior Language Model Researcher, Y Company (2022-Present)*\\n\\n* Conduct research on large language models (LLM) and their fine-tuning techniques\\n* Prototype use cases using available large language tools\\n* Mentor and guide junior researchers\\n\\n*Machine Learning Engineer, X Company (2017-2022)*\\n\\n* Developed various machine learning models for X project\\n* Mentored junior developers\\n* Deployed and monitored models in production\\n\\n**Education**\\n\\n*PhD in Computer Science, University of Toronto (2017)*\\n\\n*Master of Science in Computer Science, University of Toronto (2014)*\\n\\n*Bachelor of Science in Computer Science, University of Toronto (2012)*\\n\\n**Skills**\\n\\n* Programming Languages: Python\\n* Deep Learning Frameworks: PyTorch, TensorFlow\\n* Data Manipulation: Panda, SQL\\n* Machine Learning Libraries: Scikit-learn\\n* Mentoring and Leadership\\n\\nPlease let me know if this revised version meets your requirements. I am always here to help you.', response_metadata={'token_usage': {'prompt_tokens': 286, 'total_tokens': 1061, 'completion_tokens': 775}, 'model_name': 'accounts/fireworks/models/mixtral-8x7b-instruct', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run-feba5311-874e-4497-9e1f-e2eea8dcc46b-0')}\n",
      "\n",
      "---\n",
      "\n",
      "{'refine': HumanMessage(content='Overall, your resume is well-organized and provides a clear overview of your education and professional experience. However, there are a few areas where it could be improved to make it even stronger:\\n\\n1. Quantify your achievements: Wherever possible, try to quantify your achievements with numbers and statistics. For example, instead of saying \"developed various machine learning models,\" you could say \"developed and deployed 10 machine learning models, resulting in a 20% increase in efficiency.\"\\n2. Add a \"Publications\" or \"Presentations\" section: If you have any publications or presentations related to your research, consider adding a separate section to highlight them. This can help demonstrate your expertise and credibility in the field.\\n3. Tailor your resume to the job posting: While your resume is well-written, it\\'s important to tailor it to each job posting. Make sure to highlight the skills and experiences that are most relevant to the job you\\'re applying for.\\n4. Include a \"Languages\" section: If you are fluent in any languages other than English, consider adding a \"Languages\" section to your resume. This can be a valuable asset, particularly if you\\'re applying for a job that involves working with international teams or clients.\\n5. Consider adding a \"Professional Affiliations\" section: If you are a member of any professional organizations related to your field, consider adding a \"Professional Affiliations\" section to your resume. This can help demonstrate your commitment to your profession and your ongoing learning and development.\\n\\nIn terms of length, depth, and style, here are some specific recommendations:\\n\\n1. Length: Aim for a one-page resume if you have less than 10 years of experience. If you have more than 10 years of experience, you can expand to two pages, but try to keep it as concise as possible.\\n2. Depth: Focus on highlighting your most relevant and impressive achievements. Don\\'t include every job or project you\\'ve ever worked on - instead, choose the ones that demonstrate your skills and expertise most effectively.\\n3. Style: Use a clean and simple format with clear headings and bullet points. Use action verbs to start each bullet point, and avoid using pronouns (e.g. \"I developed...\" should be \"Developed...\"). Use a consistent font and font size throughout the resume, and make sure there are no spelling or grammar errors.\\n\\nHere\\'s an example of how you could revise your resume based on these recommendations:\\n\\n**Joe Smith**\\nPhD in Computer Science, University of Toronto (2017)\\nEmail: [joe.smith@example.com](mailto:joe.smith@example.com)\\n\\n**Professional Summary**\\nExperienced Machine Learning Engineer and Senior Language Model Researcher with a strong background in developing machine learning models, conducting research on large language models, and mentoring junior developers. Proficient in Python and deep learning frameworks such as PyTorch and TensorFlow. Skilled in using Panda and SQL for data manipulation and machine learning libraries such as Scikit-learn.\\n\\n**Professional Experience**\\n\\n*Senior Language Model Researcher, Y Company (2022-Present)*\\n\\n* Conduct research on large language models (LLM) and their fine-tuning techniques\\n* Prototype use cases using available large language tools\\n* Mentor and guide junior researchers\\n\\n*Machine Learning Engineer, X Company (2017-2022)*\\n\\n* Developed and deployed 10 machine learning models for X project, resulting in a 20% increase in efficiency\\n* Mentored junior developers, providing guidance on best practices and technical skills\\n* Deployed and monitored models in production, ensuring optimal performance and addressing any issues\\n\\n**Education**\\n\\n*PhD in Computer Science, University of Toronto (2017)*\\n\\n*Master of Science in Computer Science, University of Toronto (2014)*\\n\\n*Bachelor of Science in Computer Science, University of Toronto (2012)*\\n\\n**Skills**\\n\\n* Programming Languages: Python\\n* Deep Learning Frameworks: PyTorch, TensorFlow\\n* Data Manipulation: Panda, SQL\\n* Machine Learning Libraries: Scikit-learn\\n* Mentoring and Leadership\\n\\n**Publications**\\n\\n* \"A Study on Fine-Tuning Techniques for Large Language Models\" (2022)\\n* \"Improving Language Model Performance with Data Augmentation\" (2021)\\n\\n**Professional Affiliations**\\n\\n* Association for Computational Linguistics\\n* Institute of Electrical and Electronics Engineers (IEEE)', id='6d4128f1-c685-45c8-a2f6-f6ef74948324')}\n",
      "\n",
      "---\n",
      "\n",
      "$$$$$$$ messages length: $$$$$$$$$ 4\n",
      "{'generate': AIMessage(content='Thank you for your feedback and critique. I have revised the resume based on your suggestions:\\n\\n**Joe Smith**\\nPhD in Computer Science, University of Toronto (2017)\\nEmail: joe.smith@example.com\\n\\n**Professional Summary**\\nExperienced Machine Learning Engineer and Senior Language Model Researcher with a strong background in developing machine learning models, conducting research on large language models, and mentoring junior developers. Proficient in Python and deep learning frameworks such as PyTorch and TensorFlow. Skilled in using Panda and SQL for data manipulation and machine learning libraries such as Scikit-learn.\\n\\n**Professional Experience**\\n\\n*Senior Language Model Researcher, Y Company (2022-Present)*\\n\\n* Conduct research on large language models (LLM) and their fine-tuning techniques\\n* Prototype use cases using available large language tools\\n* Mentor and guide junior researchers\\n\\n*Machine Learning Engineer, X Company (2017-2022)*\\n\\n* Developed and deployed 10 machine learning models for X project, resulting in a 20% increase in efficiency\\n* Mentored junior developers, providing guidance on best practices and technical skills\\n* Deployed and monitored models in production, ensuring optimal performance and addressing any issues\\n\\n**Education**\\n\\n*PhD in Computer Science, University of Toronto (2017)*\\n\\n*Master of Science in Computer Science, University of Toronto (2014)*\\n\\n*Bachelor of Science in Computer Science, University of Toronto (2012)*\\n\\n**Skills**\\n\\n* Programming Languages: Python\\n* Deep Learning Frameworks: PyTorch, TensorFlow\\n* Data Manipulation: Panda, SQL\\n* Machine Learning Libraries: Scikit-learn\\n* Mentoring and Leadership\\n\\n**Publications**\\n\\n* \"A Study on Fine-Tuning Techniques for Large Language Models\" (2022)\\n* \"Improving Language Model Performance with Data Augmentation\" (2021)\\n\\n**Professional Affiliations**\\n\\n* Association for Computational Linguistics\\n* Institute of Electrical and Electronics Engineers (IEEE)\\n\\nI hope this revised version meets your requirements. Let me know if there are any further changes or additions you would like me to make.', response_metadata={'token_usage': {'prompt_tokens': 2120, 'total_tokens': 2659, 'completion_tokens': 539}, 'model_name': 'accounts/fireworks/models/mixtral-8x7b-instruct', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run-006e78a9-437a-4f8d-9eba-6692410790b5-0')}\n",
      "\n",
      "---\n",
      "\n",
      "{'refine': HumanMessage(content=\"Great job on the revised resume! You've incorporated all of my suggestions and the result is a clear, concise, and well-organized resume that effectively highlights your skills and experience.\\n\\nThe professional summary is a strong introduction to your background and expertise, and the bullet points under your professional experience provide specific examples of your achievements and contributions. The education section is also well-organized and easy to read.\\n\\nI appreciate that you've added a publications section to showcase your research, and the professional affiliations section adds credibility and demonstrates your commitment to the field.\\n\\nOverall, this is an excellent resume that effectively highlights your skills and experience. I would be happy to review any further revisions or additions you would like to make.\\n\\nWell done, and good luck with your job search!\", id='c46d6e0a-078f-4342-bc6a-e4fbf4fa501c')}\n",
      "\n",
      "---\n",
      "\n",
      "$$$$$$$ messages length: $$$$$$$$$ 6\n",
      "{'generate': AIMessage(content=\"Thank you for your kind words and feedback. I am glad that you found the revised resume to be clear, concise, and well-organized. I appreciate your suggestions, as they helped me to improve the resume and make it more effective in highlighting the user's skills and experience.\\n\\nI am glad that the professional summary, bullet points under professional experience, education section, publications section, and professional affiliations section were well-received. These sections were designed to provide a comprehensive overview of the user's background, skills, and achievements.\\n\\nThank you for your offer to review any further revisions or additions. I am always here to help and make any changes that may be necessary.\\n\\nOnce again, thank you for your positive feedback, and I wish the user the best of luck in their job search!\", response_metadata={'token_usage': {'prompt_tokens': 2839, 'total_tokens': 3014, 'completion_tokens': 175}, 'model_name': 'accounts/fireworks/models/mixtral-8x7b-instruct', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run-8c66dcac-639c-4a09-a96d-ba01f9e741e6-0')}\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from os import linesep\n",
    "input = HumanMessage(content = f\"\"\"Write a resume based on the following information: \\n\n",
    "                                Resume : {resume_transcription} \\n\n",
    "                                \"\"\")\n",
    "outputs = []\n",
    "async for event in app.astream(input):\n",
    "    outputs.append(event)\n",
    "    print(event)\n",
    "\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cdca155-cb83-4735-ada4-eddfa5cd4f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generate': AIMessage(content='Joe Smith\\n------------------\\nPhD in Computer Science - University of Toronto (2017)\\n\\nContact Information\\n------------------\\nEmail: joe.smith@example.com\\n\\nSummary Statement\\n------------------\\nExperienced machine learning engineer and language model researcher with a PhD in computer science. Proficient in Python and deep learning frameworks such as PyTorch and TensorFlow. Skilled in data manipulation using Panda and SQL, and experienced in mentoring junior developers.\\n\\nWork Experience\\n------------------\\nSenior Language Model Researcher - Y Company (2022-Present)\\n\\n* Conduct research on large language models (LLM) and develop fine-tuning and augmentation techniques\\n* Prototype use cases using available large language tools, resulting in a 30% increase in efficiency\\n\\nMachine Learning Engineer - X Company (2017-2022)\\n\\n* Developed and deployed 5 machine learning models, resulting in a 20% increase in efficiency\\n* Mentored junior developers, providing guidance and support on machine learning projects\\n* Monitored models in production using tools such as Prometheus and Grafana, ensuring optimal performance and addressing any issues\\n\\nSkills\\n------------------\\n\\n* Proficient in Python, PyTorch, and TensorFlow\\n* Experience with data manipulation tools such as Panda and SQL\\n* Familiarity with machine learning libraries such as Scikit-learn\\n\\nPublications\\n------------------\\n\\n* \"A Comparative Study of Deep Learning Frameworks for Natural Language Processing\" (2019)\\n* \"Fine-Tuning Large Language Models for Improved Performance\" (2021)\\n\\nI hope this revised version meets your needs. Let me know if there are any further changes you would like me to make.', response_metadata={'token_usage': {'prompt_tokens': 1995, 'total_tokens': 2391, 'completion_tokens': 396}, 'model_name': 'accounts/fireworks/models/mixtral-8x7b-instruct', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run-bc36ed63-b016-4ce9-b796-2980cfb032da-0')}\n"
     ]
    }
   ],
   "source": [
    "print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f011e4a-f1de-4027-a7d0-83b0dcdb918e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
