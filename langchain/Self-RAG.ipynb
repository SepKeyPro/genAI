{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyM8Y4uLW/eqk/7NXMN9bbgj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install langchain-core langchain langgraph langchain-community langchain-openai pypdf langchainhub chromadb"],"metadata":{"id":"gDmb89lbm5D7","executionInfo":{"status":"ok","timestamp":1720193529420,"user_tz":240,"elapsed":163,"user":{"displayName":"sepehr keykhaie","userId":"18194880490269398179"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LfuvaxBNmVy7","executionInfo":{"status":"ok","timestamp":1720193570026,"user_tz":240,"elapsed":10945,"user":{"displayName":"sepehr keykhaie","userId":"18194880490269398179"}},"outputId":"959d8510-1159-474f-cc1d-96ddc511ba20"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"]}],"source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.document_loaders import WebBaseLoader\n","from langchain_community.vectorstores import Chroma\n","from langchain_openai import OpenAIEmbeddings\n","from google.colab import userdata\n","api_key = userdata.get(\"OPENAI\")\n","urls = [\n","    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n","    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n","    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n","]\n","\n","docs = [WebBaseLoader(url).load() for url in urls]\n","docs_list = [item for sublist in docs for item in sublist]\n","\n","text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=250, chunk_overlap=0)\n","docs_splits = text_splitter.split_documents(docs_list)\n","# Add to vectorDB\n","vectorstore = Chroma.from_documents(\n","    documents=docs_splits,\n","    collection_name=\"rag-chroma\",\n","    embedding=OpenAIEmbeddings(api_key=api_key),\n",")\n","retriever = vectorstore.as_retriever()"]},{"cell_type":"code","source":["# Retrival grader\n","\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.pydantic_v1 import BaseModel, Field\n","from langchain_openai import ChatOpenAI\n","\n","class GradeDocument(BaseModel):\n","  binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n","\n","\n","# LLM with function call\n","llm = ChatOpenAI(api_key=api_key, model=\"gpt-4\", temperature=0)\n","structured_llm_grader = llm.with_structured_output(GradeDocument)\n","\n","# Prompt\n","system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n","    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n","    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n","    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n","grade_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", system),\n","        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n","    ]\n",")\n","\n","retrival_grader = grade_prompt | structured_llm_grader\n","\n"],"metadata":{"id":"jmkUkUOqrsDd","executionInfo":{"status":"ok","timestamp":1720193574561,"user_tz":240,"elapsed":373,"user":{"displayName":"sepehr keykhaie","userId":"18194880490269398179"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from langchain import hub\n","from langchain_core.output_parsers import StrOutputParser\n","\n","rag_prompt = hub.pull(\"rlm/rag-prompt\")\n","llm = ChatOpenAI(api_key=api_key, model=\"gpt-4\", temperature=0)\n","\n","rag_chain = rag_prompt | llm | StrOutputParser()\n","\n","question = \"agent memory\"\n","\n","docs = retriever.get_relevant_documents(question)\n","generation = rag_chain.invoke({\"question\": question, \"context\": docs})\n","print(generation)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3vP0e4tYwWO4","executionInfo":{"status":"ok","timestamp":1720201054401,"user_tz":240,"elapsed":5313,"user":{"displayName":"sepehr keykhaie","userId":"18194880490269398179"}},"outputId":"e348620a-dbc6-4aa2-9975-736f03d6e9ec"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["In a Large Language Model (LLM) powered autonomous agent system, the agent's memory is a key component. It includes both short-term and long-term memory. Short-term memory is used for in-context learning, while long-term memory, often leveraged through an external vector store and fast retrieval, allows the agent to retain and recall information over extended periods. Additionally, a memory stream, which is a long-term memory module, records a comprehensive list of the agents' experiences in natural language.\n"]}]},{"cell_type":"code","source":["# Hallucination Grader\n","\n","class HallucinationGrade(BaseModel):\n","  binary_score: str = Field(description=\"Model hallucinated in answering the question, 'yes' or 'no'\")\n","\n","# LLM with function call\n","llm = ChatOpenAI(api_key=api_key, model=\"gpt-4\", temperature=0)\n","structured_llm_grader = llm.with_structured_output(HallucinationGrade)\n","system = \"\"\"You are assessing the generation based on the facts provided in the documents. \\n\n","         Assess the generated answer. If the answer is grounded in the facts given in the documents say 'No', otherwise say 'Yes'\n","        \"\"\"\n","hallucination_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", system),\n","        (\"human\", \"Retrieved documents: \\n\\n {documents} \\n\\n generated answer: {generation}\"),\n","    ])\n","\n","hallucination_grader = hallucination_prompt | structured_llm_grader\n","hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nZIMfGiS478u","executionInfo":{"status":"ok","timestamp":1720201058819,"user_tz":240,"elapsed":1157,"user":{"displayName":"sepehr keykhaie","userId":"18194880490269398179"}},"outputId":"3e53c916-3209-4517-ccc2-b9b9fd21f8d7"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["HallucinationGrade(binary_score='No')"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["# Answer Grader\n","\n","class AnswerGrade(BaseModel):\n","  binary_score: str = Field(description=\"Model answer is correct, 'yes' or 'no'\")\n","\n","# LLM with function call\n","llm = ChatOpenAI(api_key=api_key, model=\"gpt-4\", temperature=0)\n","structured_llm_grader = llm.with_structured_output(AnswerGrade)\n","system = \"\"\"You are assessing the generated answer. \\n\n","         Assess the generated answer. If the answer addresses the question correctly say 'Yes', otherwise say 'No'.\n","        \"\"\"\n","answer_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", system),\n","        (\"human\", \"generated answer: {generation} \\n\\n question: {question}\"),\n","    ])\n","\n","answer_grader = answer_prompt | structured_llm_grader\n","\n","answer_grader.invoke({\"generation\": generation, \"question\": question})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9uS2hM1Q9-Yn","executionInfo":{"status":"ok","timestamp":1720201063152,"user_tz":240,"elapsed":1033,"user":{"displayName":"sepehr keykhaie","userId":"18194880490269398179"}},"outputId":"475fe3ff-d4e9-4674-b747-66aa28d838aa"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AnswerGrade(binary_score='Yes')"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["def retrive_docs(state):\n","  question = state[\"question\"]\n","  docs = retriever.get_relevant_documents(question)\n","  print(\"Retriving documents\")\n","  return {'docs': docs, 'question': question}"],"metadata":{"id":"ZYmQJ22K3SZ1","executionInfo":{"status":"ok","timestamp":1720201068150,"user_tz":240,"elapsed":198,"user":{"displayName":"sepehr keykhaie","userId":"18194880490269398179"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["def grade_documents(state):\n","  question = state[\"question\"]\n","  docs = state[\"docs\"]\n","  generation = state[\"generation\"]\n","  filtered_docs = []\n","  for doc in docs:\n","    grade = retrival_grader.invoke({\"question\": question, \"document\": doc})\n","    if grade.binary_score == \"yes\":\n","      print(\"Doc is relevant, adding to filtered docs list\")\n","      filtered_docs.append(doc)\n","    else:\n","      print(\"Doc is not relevant, going to the next one\")\n","      pass\n","  return {'docs': filtered_docs, 'question': question}"],"metadata":{"id":"_41L0fcCGmT8","executionInfo":{"status":"ok","timestamp":1720201070515,"user_tz":240,"elapsed":191,"user":{"displayName":"sepehr keykhaie","userId":"18194880490269398179"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["def generate(state):\n","  question = state[\"question\"]\n","  docs = state[\"docs\"]\n","  print(\"Generating answer\")\n","  generation = rag_chain.invoke({\"question\": question, \"context\": docs})\n","  return {'generation': generation, 'question': question}"],"metadata":{"id":"vToZIlhXECLB","executionInfo":{"status":"ok","timestamp":1720201072493,"user_tz":240,"elapsed":185,"user":{"displayName":"sepehr keykhaie","userId":"18194880490269398179"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["def decide_to_generate(state):\n","  question = state[\"question\"]\n","  filtered_docs = state[\"docs\"]\n","  if not filtered_docs:\n","    print(\"No relevant doc\")\n","    return 'not_useful'\n","  else:\n","    return 'generate'"],"metadata":{"id":"hJw0ZE2fKASR","executionInfo":{"status":"ok","timestamp":1720201074813,"user_tz":240,"elapsed":178,"user":{"displayName":"sepehr keykhaie","userId":"18194880490269398179"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["def grade_generation_v_documents_and_question(state):\n","  question = state[\"question\"]\n","  docs = state[\"docs\"]\n","  generation = state[\"generation\"]\n","  hall_grade = hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})\n","  if hall_grade.binary_score == \"No\":\n","    answer_grade = answer_grader.invoke({\"generation\": generation, \"question\": question})\n","    if answer_grade.binary_score == \"Yes\":\n","      return 'useful'\n","    else:\n","      return 'not_useful'\n","  else:\n","    return 'not_supported'"],"metadata":{"id":"6-l1nFNPOu5h","executionInfo":{"status":"ok","timestamp":1720201076513,"user_tz":240,"elapsed":192,"user":{"displayName":"sepehr keykhaie","userId":"18194880490269398179"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i0UlIvlSKwHk","executionInfo":{"status":"ok","timestamp":1720204526328,"user_tz":240,"elapsed":20750,"user":{"displayName":"sepehr keykhaie","userId":"18194880490269398179"}},"outputId":"4203994a-3659-4d12-c5ff-d82125197908"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd drive/MyDrive/MyRepo/genAI/langchain\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oy8aiMIbLhuZ","executionInfo":{"status":"ok","timestamp":1720204565571,"user_tz":240,"elapsed":169,"user":{"displayName":"sepehr keykhaie","userId":"18194880490269398179"}},"outputId":"541decff-38d4-459e-b834-015e1c2c2bcf"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/MyRepo/genAI/langchain\n"]}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dqVtDBQFLqfa","executionInfo":{"status":"ok","timestamp":1720204573098,"user_tz":240,"elapsed":152,"user":{"displayName":"sepehr keykhaie","userId":"18194880490269398179"}},"outputId":"5ff39e14-934f-434c-d93a-87b662532657"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":[" chatbot_crag.ipynb   crag.ipynb\t     resume_specialist.ipynb   sql_agent_with_memory.py\n"," chatbot_rag.ipynb    job_match.ipynb\t     Self-RAG.ipynb\t       vector_stores\n"," code_rag.ipynb      'Query Routing.ipynb'   sql_agent_no_memory.py\n"]}]},{"cell_type":"markdown","source":["# Graph\n","\n","<center><figure><img src=\"../imgs/self-rag.png\" alt=\"drawing\" width=\"1100\"/><figcaption>Fig. 1: Graph diagram</figcaption></figure></center>"],"metadata":{"id":"SaG6xmVHFG1S"}},{"cell_type":"code","source":["from typing import List, TypedDict\n","\n","class GraphSate(TypedDict):\n","  question: str\n","  docs: List[str]\n","  generation: str\n","\n"],"metadata":{"id":"Vdp2xBgvAOZ5","executionInfo":{"status":"ok","timestamp":1720201065169,"user_tz":240,"elapsed":202,"user":{"displayName":"sepehr keykhaie","userId":"18194880490269398179"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["from langgraph.graph import END, StateGraph, START\n","\n","workflow = StateGraph(GraphSate)\n","workflow.add_node('retrive', retrive_docs)\n","workflow.add_node('generate', generate)\n","workflow.add_node('grade_documents', grade_documents)\n","\n","workflow.add_edge(START, 'retrive')\n","workflow.add_edge('retrive', 'grade_documents')\n","workflow.add_conditional_edges(\n","    'grade_documents',\n","    decide_to_generate,\n","    {\n","        \"not_useful\": END,\n","        \"generate\": \"generate\",\n","    },\n",")\n","\n","workflow.add_conditional_edges(\n","    'generate',\n","    grade_generation_v_documents_and_question,\n","    {\n","        \"not_supported\": \"generate\",\n","        \"not_useful\": END,\n","        \"useful\": END,\n","    },\n",")\n","\n","app = workflow.compile()"],"metadata":{"id":"s1XeFHsBPXd5","executionInfo":{"status":"ok","timestamp":1720201078848,"user_tz":240,"elapsed":175,"user":{"displayName":"sepehr keykhaie","userId":"18194880490269398179"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":["# Output\n","\n","Try with a question relevant to our index."],"metadata":{"id":"VRnktitxCtC0"}},{"cell_type":"code","source":["from pprint import pprint\n","\n","# Run\n","inputs = {\"question\": \"What is agent memory?\"}\n","for output in app.stream(inputs):\n","    for key, value in output.items():\n","        # Node\n","        pprint(f\"Node '{key}':\")\n","        # Optional: print full state at each node\n","        # pprint(f\"value: {value}\")\n","    pprint(\"\\n---\\n\")\n","# Final generation\n","response = value.get('generation')\n","if response is None:\n","    print(\"Couldn't generate a uselful answer\")\n","else:\n","    pprint(response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T9Ocr-XzrfsL","executionInfo":{"status":"ok","timestamp":1720202201420,"user_tz":240,"elapsed":10025,"user":{"displayName":"sepehr keykhaie","userId":"18194880490269398179"}},"outputId":"81f8a7ef-222c-4872-90b1-297837327939"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["Retriving documents\n","\"Node 'retrive':\"\n","'\\n---\\n'\n","Doc is relevant, adding to filtered docs list\n","Doc is relevant, adding to filtered docs list\n","Doc is relevant, adding to filtered docs list\n","Doc is not relevant, going to the next one\n","\"Node 'grade_documents':\"\n","'\\n---\\n'\n","Generating answer\n","\"Node 'generate':\"\n","'\\n---\\n'\n","('Agent memory in the context of Large Language Model (LLM) powered autonomous '\n"," 'agents refers to the capability of the agent to retain and recall '\n"," 'information over time. This can be divided into short-term and long-term '\n"," 'memory. Short-term memory is used for in-context learning, while long-term '\n"," 'memory allows the agent to retain and recall information over extended '\n"," 'periods, often by leveraging an external vector store and fast retrieval.')\n"]}]},{"cell_type":"markdown","source":["Try with a question not relevant to our index"],"metadata":{"id":"T4y1Wl9GDgn3"}},{"cell_type":"code","source":["# Run\n","inputs = {\"question\": \"How many states does U.S.A have?\"}\n","for output in app.stream(inputs):\n","    for key, value in output.items():\n","        # Node\n","        pprint(f\"Node '{key}':\")\n","        # Optional: print full state at each node\n","        # pprint(f\"value: {value}\")\n","    pprint(\"\\n---\\n\")\n","# Final generation\n","response = value.get('generation')\n","if response is None:\n","    print(\"Couldn't generate a uselful answer\")\n","else:\n","    pprint(response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KAmvr64-Dnjv","executionInfo":{"status":"ok","timestamp":1720202484041,"user_tz":240,"elapsed":4035,"user":{"displayName":"sepehr keykhaie","userId":"18194880490269398179"}},"outputId":"16cb0a17-21cb-4453-f5f5-e5ef20bbbdff"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["Retriving documents\n","\"Node 'retrive':\"\n","'\\n---\\n'\n","Doc is not relevant, going to the next one\n","Doc is not relevant, going to the next one\n","Doc is not relevant, going to the next one\n","Doc is not relevant, going to the next one\n","No relevant doc\n","\"Node 'grade_documents':\"\n","'\\n---\\n'\n","Couldn't generate a uselful answer\n"]}]}]}