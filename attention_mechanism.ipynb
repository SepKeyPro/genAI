{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1c6db12-454c-4ad4-8d39-162dbbb751cf",
   "metadata": {},
   "source": [
    "# What is happening at the heat of LLMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eba597b-1b08-4ee5-a483-921d3e99c025",
   "metadata": {},
   "source": [
    "Before transformers, recurrent neural networks (RNNs) were considered the cutting edge in Natural Language Processing (NLP). An RNN is a type of neural network where outputs from previous steps are fed as inputs to the current step. This characteristic enables an RNN to retain information from previous steps, making them well-suited for sequential data like text. In the context of NLP, an RNN takes an input, such as a word or character, processes it through its network, and generates a vector known as the hidden state. If you are unfamiliar with RNNs, don't worry, you don't need to know the detailed workings of RNNs to follow this discussion. \n",
    "\n",
    "One area where RNNs played an important role was in the development of machine translation systems, where the model translates text from one language to another. However, the word sequence in one language might be different from another one due to the grammatical structures in the source and target language. To address this issue we can use an encoder-decoder architecture. The encoder's role is to convert input sequence information into a numerical representation, typically referred to as the final hidden state. The encoder updates its hidden state at each step, trying to capture the entire meaning of the input sentence in the final hidden state. The decoder then takes this final hidden state to start generating the translated sentence, one word at a time. \\\n",
    "However, a significant challenge of this architecture lies in the fact that the final hidden state of the encoder creates an information bottleneck. it has to represent the meaning of the whole input sequence because this is all the decoder has access to when generating the output. This is especially challenging for long sequences, where information at the start of the sequence might be lost in the process of compressing everything to a single, fixed representation.\n",
    "\n",
    "To address this challenge, an \"attention mechanism\" is introduced, permitting the decoder to selectively access different hidden states of the encoder. But, why selective? Using all the states at the same time would create a huge input for the decoder, the attention mechanism lets the decoder assign a different amount of weight, or \"attention\" to each of the encoder states at every decoding timestep. \\\n",
    "Researchers, as detailed in the paper \"Attention is all you need,\" have demonstrated that RNN architectures are not required for NLP applications such as machine translation and proposed a transformer architecture with a “self-attention mechanism”.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff54b0f2-c383-425a-9f3a-1c2edd3ac0a5",
   "metadata": {},
   "source": [
    "The main idea behind the self-attention mechanism is that instead of using fixed embeddings for each token, we can use the whole sequence to compute a weighted average of each embedding. Given a sequence of token embeddings $ x_{1}, ..., x_{n} $ self-attention produces a sequence of new embeddings $ x_{1}^{'}, ..., x_{n}^{'} $ where each $ x_{i}^{'} $ is a linear combination of all the $ x_{j}^{'},  j=1...n $: \n",
    "\n",
    "$(1) \\; x_{i} = \\sum \\limits _{j=1} ^{n} w_{ji}x_{j}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88efa144-ee58-4731-9bc4-cdde2840e6f3",
   "metadata": {},
   "source": [
    "There are several ways to implement a self-attention layer. The original implementation  introduced in the paper “” is called \"Scaled Dot-Product Attention\" \n",
    "\n",
    "$ (2) \\; Attention(Q,K,V) =  softmax(\\frac{QK^{T}}{\\sqrt{d_{k}}})V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd01043-9dd1-4900-acc1-974c7c8893fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "690846d4-0197-4e18-ade7-9e046beb10ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471a2c97-efa1-41e7-9bf6-558e800ac342",
   "metadata": {},
   "source": [
    "Putting these steps together, we will have the following function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e36dfc-1c3a-4ecf-920c-601248af4060",
   "metadata": {},
   "source": [
    "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. \n",
    "\n",
    "$(3) \\; MultiHead(Q,K,V) =  Concat(head_{1}, ..., head_{h})W^{O} $ \n",
    "\n",
    "where, \n",
    "\n",
    "$ head_{i} = Attention(QW_{i}^{Q},KW_{i}^{K},VW{i}^{V})$ and, \\\n",
    "$ W_{i}^{Q} \\in \\mathbb{R}^{d_{model}\\times d_{k}}, W_{i}^{K} \\in \\mathbb{R}^{d_{model} \\times d_{k}}, W_{i}^{V} \\in \\mathbb{R}^{d_{model} \\times d_{v}} and W^{O} \\in \\mathbb{R}^{d_{hv} \\times d_{model}} $ are weight matrices.\n",
    "\n",
    "These three weight matrices are used to project the embedded input tokens, x(i), into query, key, and value vectors.\n",
    "\n",
    "These matrices transform input data into queries, keys, and values, which are crucial components of the attention mechanism. As the model is exposed to more data during training, it adjusts these trainable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a703e4b-206a-44cf-a9f1-01d6a2d6c18f",
   "metadata": {},
   "source": [
    "# What are Query, Key and Value ? \n",
    "\n",
    "In attention mechanisms, we use terms like \"key,\" \"query,\" and \"value\" which come from information retrieval and databases. They help us store, search, and get information efficiently.\n",
    "\n",
    "Think of a \"query\" like a search term you put into a database. It's what the model is currently focusing on or trying to understand, like a word in a sentence. The query helps the model figure out how much attention to give to other parts of the input.\n",
    "\n",
    "A \"key\" is like an index in a database used for searching. Each item in the input sequence, such as each word in a sentence, has a key. These keys are matched with the query to find relevant information.\n",
    "\n",
    "The \"value\" in this context is similar to the value in a key-value pair in a database. It represents the actual content or representation of the input items. Once the model determines which keys (and thus which parts of the input) are most relevant to the query (the current focus item), it retrieves the corresponding values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2ca443-6607-4524-b016-b89006399574",
   "metadata": {},
   "source": [
    "With this introduction, let's code our very first attention mechanism. Imagine we have an embedding model that generates embeddings in a 5 dimentional embedding space. Assume that our embedding model has generated the following embedding vectors for our input sentence \"Write your first Attention mechanism\".  \n",
    "\n",
    "Please note that embedding values in this example are totally random and dosen't express any information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "94041d69-e0b0-4532-81e3-c2f807213b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Write your first Attention mechanism\n",
    "inputs = torch.tensor(\n",
    "  [[0.172, 0.295, 0.618, 0.459, 0.818, 0.071], # Write \n",
    "   [0.265, 0.563, 0.718, 0.323, 0.126, 0.235], # your                                               \n",
    "   [0.206, 0.333, 0.044, 0.862, 0.152, 0.594], # first    \n",
    "   [0.300, 0.505, 0.727, 0.495, 0.898, 0.954], # Attention     \n",
    "   [0.095, 0.809, 0.596, 0.110, 0.447, 0.418]] # mechanism   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bd2ba4a9-a682-4660-b75f-3afbee5720e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 6])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f256b66-a73d-473a-b6f0-b20329f3bff2",
   "metadata": {},
   "source": [
    "First we should generate attention weight scores, which simply is the dot product of each embedding vector with other embedding vecotrs. Dot product is used as a similarity function. \n",
    "\n",
    "That is $ Attention Scores \\in \\mathbb{R}^{d_{t}\\times d_{t}} $ \\\n",
    "Where $ d_{t} $ is the number of input tokens (i.e., words), here $ d_{t} = 5 $   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7281b94c-a683-4e27-be61-16b37a0ab9f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention scores are:  tensor([[1.3834, 0.9234, 0.7230, 1.6794, 1.0691],\n",
      "        [0.9234, 1.0781, 0.7108, 1.3830, 1.0987],\n",
      "        [0.7230, 0.7108, 1.2742, 1.3918, 0.7262],\n",
      "        [1.6794, 1.3830, 1.3918, 2.8351, 1.7250],\n",
      "        [1.0691, 1.0987, 0.7262, 1.7250, 1.4054]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.matmul(inputs, inputs.T) \n",
    "print(\"attention scores are: \", attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d76a8c-3944-4af0-af77-4a06a5829c7c",
   "metadata": {},
   "source": [
    "Then, we normalize the attention scores using softmax function. The main goal behind the normalization is to obtain attention weights that sum up to 1. \n",
    "\n",
    "$ (4) \\; \\sigma(\\mathbf{z})_{i} = \\frac{e^{z_{i}}}{\\sum_{j=1}^{K} e^{z_{j}}}, for \\; i = 1 ,...,K \\; and \\; \\mathbf{z} \\in \\mathbb{R}^{K} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fec6e41b-7914-4a8a-9a0c-16137a4665f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights are:  tensor([[0.2368, 0.1495, 0.1224, 0.3184, 0.1730],\n",
      "        [0.1739, 0.2030, 0.1406, 0.2753, 0.2072],\n",
      "        [0.1497, 0.1479, 0.2598, 0.2923, 0.1502],\n",
      "        [0.1489, 0.1107, 0.1117, 0.4729, 0.1558],\n",
      "        [0.1649, 0.1698, 0.1170, 0.3176, 0.2307]])\n",
      "Sums of all rows are:  tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "print(\"attention weights are: \", attn_weights)\n",
    "print(\"Sums of all rows are: \",attn_weights.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "169d59a4-2388-410f-8542-586ec4b164ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2175, 0.4955, 0.5936, 0.4391, 0.5944, 0.5007],\n",
      "        [0.2149, 0.5191, 0.5831, 0.4257, 0.5291, 0.4928],\n",
      "        [0.2204, 0.4831, 0.5122, 0.5017, 0.5102, 0.5414],\n",
      "        [0.2346, 0.5083, 0.6131, 0.4516, 0.6470, 0.6192],\n",
      "        [0.2147, 0.5302, 0.5974, 0.4140, 0.5624, 0.5206]])\n"
     ]
    }
   ],
   "source": [
    "context_vectors = torch.matmul(attn_weights, inputs)  \n",
    "print(context_vectors) # 5x6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4890497-c103-46ae-b82c-f73d8f21b819",
   "metadata": {},
   "source": [
    "As you can see, context vectors are the same size as our inputs. In other way, we simply modified the embeddings to reflect the attention to other tokens as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cf12ea-923a-407b-ad78-67ffb24fea88",
   "metadata": {},
   "source": [
    "# Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c644237b-9a7b-4f25-bcec-91700d1fb94d",
   "metadata": {},
   "source": [
    "The paper \"Attention Is All You Need\" introduces <em> Scaled Dot-Product Attention </em>. For instance, when scaling up the embedding dimension, which is typically greater than thousand for GPT-like LLMs, large dot products can result in very small gradients during backpropagation due to the softmax function applied to them. As dot products increase, the softmax function behaves more like a step function, resulting in gradients nearing zero. These small gradients can drastically slow down learning. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. \n",
    "\n",
    "<center><figure><img src=\"imgs/scaled-dot-product.png\" alt=\"drawing\" width=\"300\"/><figcaption>Fig. 1: Scaled Dot-Product Attention.</figcaption></figure></center>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb355a1-e229-4655-8c9d-7763ecfd6d26",
   "metadata": {},
   "source": [
    "Let's see figure 1 in code. First, we write a function to implement scaled dot-product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "671a0a1e-c96f-46f6-9d59-361998907582",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    dim_k = K.size(-1)\n",
    "    attn_scores = torch.matmul(Q, K.T)\n",
    "    attn_weights = torch.softmax(attn_scores / sqrt(dim_k),dim=-1)\n",
    "    return torch.matmul(attn_weights,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8e1fbc10-a6d8-4576-8a53-0a5d7344caa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "        self.W_k = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "        self.W_v = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys = self.W_k(x)\n",
    "        queries = self.W_q(x)\n",
    "        values = self.W_v(x)        \n",
    "  \n",
    "        attention_outputs = scaled_dot_product_attention(queries, keys, values)\n",
    "        return attention_outputs    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cf0696-88e1-494f-8389-d3a9e5427574",
   "metadata": {},
   "source": [
    "# Multi-head attention\n",
    "As we saw, the self-attention mechanism employs three independent linear transformations on each embedding to produce the query, key, and value vectors. Each projection has  its own set of trainable parameters, so that the model, especially the self-attention layer, can attend to various semantic features within the sequence and  learn to produce \"good\" context vectors.\t\t \t \t \t\t\n",
    "\t\t\t\n",
    "\n",
    "Additionally, it would be advantageous to incorporate multiple sets of linear projections, referred to as  “attention head”. But why several attention heads? The reason is the softmax function of a single head focuses on one similarity aspect. By employing several heads, the model simultaneously focuses on multiple aspects of similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef2ec58-34f9-482c-8649-6468713539b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><figure><img src=\"imgs/multi-head-attention.png\" alt=\"drawing\" width=\"350\"/><figcaption>Fig. 2: Multi-head Attention.</figcaption></figure></center>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610edecb-de5b-4272-8044-e05fd52ad3c3",
   "metadata": {},
   "source": [
    "Now that we have implemented self-attention mechanism, let's move forward and implement multi-head attention mechanism according to figure 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b6538be4-7a11-4264-8132-eb94d632aaf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size is: 6, head dimension is: 3 and number of heads are: 2\n"
     ]
    }
   ],
   "source": [
    "embed_size = inputs.shape[1]\n",
    "num_heads = 2\n",
    "head_dim = embed_size // num_heads\n",
    "print(f\"Embedding size is: {embed_size}, head dimension is: {head_dim} and number of heads are: {num_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e70d119e-df46-4856-9fa5-3b765ae41b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, head_dim):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SelfAttention(embed_size, head_dim) for _ in range(num_heads)])\n",
    "        self.output_linear = nn.Linear(embed_size, embed_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = torch.cat([head(inputs) for head in self.heads], dim=-1)\n",
    "        x = self.output_linear(x)\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc2f2f5-59ea-4f95-abc0-048945ed130a",
   "metadata": {},
   "source": [
    "Please note that the final linear layer is used to produce a tensor of the same size as our input tensor (i.e., 5x6 in our example). Let's check it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4595f4c7-7a73-4ab5-854e-b827bfc47ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 6])\n"
     ]
    }
   ],
   "source": [
    "mult_head_attn = MultiHeadAttention(embed_size,num_heads,head_dim)\n",
    "attn_output = mult_head_attn(inputs)\n",
    "print(attn_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3e862c-b98a-42a6-a970-568d2e512046",
   "metadata": {},
   "source": [
    "As you can see it generates an output tensor with the expected size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330892fb-a3c2-498d-912e-5109ab84abe8",
   "metadata": {},
   "source": [
    "If we look at the model architecture depicted in figure 3, there is a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "$(5) \\; FFN(x) = max(0, xW_{1} + b_{1})W_{2} + b_{2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf286d69-dc52-476a-91c8-a5a89dc0d19a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><figure><img src=\"imgs/transformer architecture.png\" alt=\"drawing\" width=\"400\"/><figcaption>Fig. 3: Transformer architecture.</figcaption></figure></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5ec97ccf-d64d-4914-a14d-63c50d22551b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ff_hidden_size = 4 * embed_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "323b6892-ac00-4f66-8a47-da9f0e86a998",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self, embed_size, ff_hidden_size):\n",
    "        super().__init__()\n",
    "        self.x1 = nn.Linear(embed_size, ff_hidden_size)\n",
    "        self.x2 = nn.Linear(ff_hidden_size, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.x1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.x2(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba336bba-6e31-4c6c-a129-60ca640e9bfd",
   "metadata": {},
   "source": [
    "Moreover, a residual connection is employed around each of the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is\n",
    "$ LayerNorm(x + Sublayer(x)) $, where $ Sublayer(x) $ is the function implemented by the sub-layer itself [1].\n",
    "\n",
    "Note: As a rule of thumb, the size of first layer is four times the size of the embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b1a91120-c000-429e-8e4c-0f263ee0cbf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_forward = FeedForwardLayer(embed_size, ff_hidden_size)\n",
    "ff_out = feed_forward(inputs)\n",
    "ff_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "62260c30-6c51-41b5-9956-e2d4017ffeb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, head_dim, ff_hidden_size):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.multi_head_attn = MultiHeadAttention(embed_size,num_heads,head_dim)\n",
    "        self.feed_forward = FeedForwardLayer(embed_size,ff_hidden_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.norm1(x + self.multi_head_attn(x))\n",
    "        x = self.norm2( x + self.feed_forward(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e38cb9-859e-46d7-a47c-7d3b1c1f865e",
   "metadata": {},
   "source": [
    "Now, we can easily add the pieces together and build the encoder part as shown in the left side of figure 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c3ebd0b2-f160-4215-bfff-738d12eaa35d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(embed_size,num_heads,head_dim,ff_hidden_size)\n",
    "encoder_output = encoder(inputs)\n",
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aefa8d-51d0-449b-b106-269401c37d4d",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "Positional encoding augments the token embeddings by adding position-dependent information about the tokens in a sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. \n",
    "\n",
    "An easy solution could be to start token positions from 0 and continue untill all tokens are counted. However, this solution may end up with very large numbers in a large sequence of tokens. Instead, a positional encoding vector is created for each position, meaning a positional encoding matrix can be created to represent all the possible positions a word can take. \n",
    "\n",
    "There are many choices of positional encodings, the authors in [1] used the sine and cosine functions to generate a unique vector for each position in the sequence. \n",
    "\n",
    "$ PE(k, 2i) = sin(\\frac {pos}{1000^{\\frac{2i}{d_{embed}}}})  $\n",
    "\n",
    "$ PE(k, 2i+1) = cos(\\frac {pos}{1000^{\\frac{2i+1}{d_{embed}}}})  $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185c47e1-4235-44b5-851a-e0b45587fc30",
   "metadata": {},
   "source": [
    "# References: \n",
    "\n",
    "[1] Ashish Vaswan, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin. \"Attention Is All You Need\". \tarXiv:1706.03762v7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32457f41-99c2-409d-b138-ed6084075c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
