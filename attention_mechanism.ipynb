{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1c6db12-454c-4ad4-8d39-162dbbb751cf",
   "metadata": {},
   "source": [
    "# What is happening at the heat of LLMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eba597b-1b08-4ee5-a483-921d3e99c025",
   "metadata": {},
   "source": [
    "Before transformers, recurrent neural networks (RNNs) were considered the cutting edge in Natural Language Processing (NLP). An RNN is a type of neural network where outputs from previous steps are fed as inputs to the current step. This characteristic enables an RNN to retain information from previous steps, making them well-suited for sequential data like text. In the context of NLP, an RNN takes an input, such as a word or character, processes it through its network, and generates a vector known as the hidden state. If you are unfamiliar with RNNs, don't worry, you don't need to know the detailed workings of RNNs to follow this discussion. \n",
    "\n",
    "One area where RNNs played an important role was in the development of machine translation systems, where the model translates text from one language to another. However, the word sequence in one language might be different from another one due to the grammatical structures in the source and target language. To address this issue we can use an encoder-decoder architecture. The encoder's role is to convert input sequence information into a numerical representation, typically referred to as the final hidden state. The encoder updates its hidden state at each step, trying to capture the entire meaning of the input sentence in the final hidden state. The decoder then takes this final hidden state to start generating the translated sentence, one word at a time. \\\n",
    "However, a significant challenge of this architecture lies in the fact that the final hidden state of the encoder creates an information bottleneck. it has to represent the meaning of the whole input sequence because this is all the decoder has access to when generating the output. This is especially challenging for long sequences, where information at the start of the sequence might be lost in the process of compressing everything to a single, fixed representation.\n",
    "\n",
    "To address this challenge, an \"attention mechanism\" is introduced, permitting the decoder to selectively access different hidden states of the encoder. But, why selective? Using all the states at the same time would create a huge input for the decoder, the attention mechanism lets the decoder assign a different amount of weight, or \"attention\" to each of the encoder states at every decoding timestep. \\\n",
    "Researchers, as detailed in the paper \"Attention is all you need,\" have demonstrated that RNN architectures are not required for NLP applications such as machine translation and proposed a transformer architecture with a “self-attention mechanism”.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff54b0f2-c383-425a-9f3a-1c2edd3ac0a5",
   "metadata": {},
   "source": [
    "The main idea behind the self-attention mechanism is that instead of using fixed embeddings for each token, we can use the whole sequence to compute a weighted average of each embedding. Given a sequence of token embeddings $ x_{1}, ..., x_{n} $ self-attention produces a sequence of new embeddings $ x_{1}^{'}, ..., x_{n}^{'} $ where each $ x_{i}^{'} $ is a linear combination of all the $ x_{j}^{'},  j=1...n $: \n",
    "\n",
    "$(1) \\; x_{i} = \\sum \\limits _{j=1} ^{n} w_{ji}x_{j}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b030006-1cbe-4c4f-9093-55cca51b894e",
   "metadata": {},
   "source": [
    "With this introduction, let's dive into implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd01043-9dd1-4900-acc1-974c7c8893fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "690846d4-0197-4e18-ade7-9e046beb10ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2ca443-6607-4524-b016-b89006399574",
   "metadata": {},
   "source": [
    "Imagine we have an embedding model that generates embeddings in a 6 dimensional embedding space. Assume that our embedding model has generated the following embedding vectors for our input sentence \"Write your first Attention mechanism\".\n",
    "\n",
    "Please note that embedding values in this example are totally random and don't express any information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94041d69-e0b0-4532-81e3-c2f807213b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your first Attention mechanism\n",
    "input_embeddings = torch.tensor(\n",
    "  [[0.172, 0.295, 0.618, 0.459, 0.818, 0.071], # Write \n",
    "   [0.265, 0.563, 0.718, 0.323, 0.126, 0.235], # your                                               \n",
    "   [0.206, 0.333, 0.044, 0.862, 0.152, 0.594], # first    \n",
    "   [0.300, 0.505, 0.727, 0.495, 0.898, 0.954], # Attention     \n",
    "   [0.095, 0.809, 0.596, 0.110, 0.447, 0.418]] # mechanism   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd2ba4a9-a682-4660-b75f-3afbee5720e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 6])\n"
     ]
    }
   ],
   "source": [
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf1b32c-162e-48e6-b763-88dda09e6101",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "Positional encoding augments the token embeddings by adding position-dependent information about the tokens in a sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41f5de6-28c8-424f-a01d-dc92e3edc11f",
   "metadata": {},
   "source": [
    "An easy solution could be to start token positions from 0 and continue until all tokens are counted. However, this solution may end up with very large numbers in a large sequence of tokens. Instead, a positional encoding vector is created for each position, meaning a positional encoding matrix can be created to represent all the possible positions a word can take. \n",
    "\n",
    "There are many choices of positional encodings, the authors in [1] used the sine and cosine functions to generate a unique vector for each position in the sequence. \n",
    "\n",
    "\n",
    "$ (2) \\; PE(pos, 2i) = sin(\\frac {pos}{10000^{\\frac{2i}{d_{e}}}}), \\; PE(pos, 2i+1) = cos(\\frac {pos}{10000^{\\frac{2i+1}{d_{e}}}})  $\n",
    "\n",
    "\n",
    "In above, $ pos = 0,...,d_{l} $ and $ i = 0,...,\\lfloor d_{e}/2 \\rfloor$ where, $ d_{l} $ is the length of input tokens and $ d_{e} $ is size of embeddings. In this example we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df8108c0-be58-4153-b845-5756cdca8eca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_l is: 5 and d_e is: 6\n"
     ]
    }
   ],
   "source": [
    "d_l = input_embeddings.size(0)\n",
    "d_e = input_embeddings.size(1)\n",
    "print(f\"d_l is: {d_l} and d_e is: {d_e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44a470d2-196c-48e9-8201-2d31f9b749f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 6])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def calculate_pe(d_l,d_e,n):\n",
    "    pe = torch.zeros(d_l,d_e)\n",
    "    for pos in range(d_l):\n",
    "        for i in range(d_e //2):\n",
    "            denominator = pos / (n**(2*i/d_e)) \n",
    "\n",
    "            pe[pos, 2*i] = math.sin(denominator) \n",
    "            pe[pos, 2*i+1] = math.cos(denominator)\n",
    "    return pe\n",
    "\n",
    "position_embeddings = calculate_pe(d_l,d_e,10000)\n",
    "print(position_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6629e0d-84e3-4686-b825-087dfbc1a472",
   "metadata": {},
   "source": [
    "If you see in the equation, for $ 2i $ and $ 2i+1 $ we have a same denominator which is $ 10000^{\\frac{2i}{d_{embed}}} $. Therefore, we can pre-calculate them in order to improve computation performance in large sequences. Moreover, we can reformulate the denominator as $ e^{- \\frac{2i \\; log(10000)}{d_{embed}}} $. For the sake of simplicity, we will not go through this implementation, however, you can refer to [2] for more details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf45cd0c-41dc-4859-a020-d2e9bb31e6c9",
   "metadata": {},
   "source": [
    "Finally, we add the position embedding to the input embedding. Since they can be the same size, so that the two can be summed. See figure 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af025e2-12f9-49c4-a638-1f56d725f470",
   "metadata": {},
   "source": [
    "<center><figure><img src=\"imgs/positional_encoding.png\" alt=\"drawing\" width=\"300\"/><figcaption>Fig. 1: Positional Encoding.</figcaption></figure></center>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fb96f85-98fe-4c81-8b7d-8cbec9b11afc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = input_embeddings + position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f128b671-8b9a-42ea-aba1-5b46233f24da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pytorch implementation\n",
    "# position = torch.arange(seq_length).unsqueeze(1)\n",
    "# div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "# print(div_term)\n",
    "# pe = torch.zeros(seq_length, 1, d_model)\n",
    "# print(pe.shape)\n",
    "# pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "# pe[:, 0, 1::2] = torch.cos(position * div_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f256b66-a73d-473a-b6f0-b20329f3bff2",
   "metadata": {},
   "source": [
    "First we should generate attention scores, which simply is the dot product of each embedding vector with other embedding vectors. Dot product is used as a similarity function.\n",
    "\n",
    "That is $ Attention Scores \\in \\mathbb{R}^{d_{l}\\times d_{l}} $ \\\n",
    "as mentioned above $ d_{l} $ is the number of input tokens (i.e., words), here $ d_{l} = 5 $   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7281b94c-a683-4e27-be61-16b37a0ab9f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention scores are:  tensor([[6.0334, 5.4477, 4.7140, 4.9825, 4.0555],\n",
      "        [5.4477, 6.3150, 5.6911, 5.1074, 3.2900],\n",
      "        [4.7140, 5.6911, 7.2858, 6.6659, 3.7172],\n",
      "        [4.9825, 5.1074, 6.6659, 8.0217, 5.1145],\n",
      "        [4.0555, 3.2900, 3.7172, 5.1145, 4.4839]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.matmul(embeddings, embeddings.T) \n",
    "print(\"attention scores are: \", attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d76a8c-3944-4af0-af77-4a06a5829c7c",
   "metadata": {},
   "source": [
    "Then, we normalize the attention scores using softmax function. The main goal behind the normalization is to obtain attention weights that sum up to 1. \n",
    "\n",
    "$ (3) \\; \\sigma(\\mathbf{z})_{i} = \\frac{e^{z_{i}}}{\\sum_{j=1}^{K} e^{z_{j}}}, for \\; i = 1 ,...,K \\; and \\; \\mathbf{z} \\in \\mathbb{R}^{K} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fec6e41b-7914-4a8a-9a0c-16137a4665f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights are:  tensor([[0.4325, 0.2408, 0.1156, 0.1512, 0.0598],\n",
      "        [0.1824, 0.4341, 0.2326, 0.1298, 0.0211],\n",
      "        [0.0414, 0.1100, 0.5418, 0.2915, 0.0153],\n",
      "        [0.0338, 0.0383, 0.1822, 0.7070, 0.0386],\n",
      "        [0.1516, 0.0705, 0.1081, 0.4371, 0.2327]])\n",
      "Sums of all rows are:  tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "print(\"attention weights are: \", attn_weights)\n",
    "print(\"Sums of all rows are: \",attn_weights.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "169d59a4-2388-410f-8542-586ec4b164ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vectors are:  tensor([[ 0.4969,  0.7521,  0.6448,  1.4542,  0.5668,  1.3252],\n",
      "        [ 0.8145,  0.6362,  0.6052,  1.4879,  0.3682,  1.3858],\n",
      "        [ 0.8516, -0.0091,  0.4480,  1.6620,  0.4033,  1.6351],\n",
      "        [ 0.5378, -0.2659,  0.7174,  1.5309,  0.7181,  1.8102],\n",
      "        [ 0.2635,  0.0893,  0.7225,  1.4187,  0.6513,  1.6058]])\n"
     ]
    }
   ],
   "source": [
    "context_vectors = torch.matmul(attn_weights, embeddings)  \n",
    "print(\"context_vectors are: \",context_vectors) # 5x6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4890497-c103-46ae-b82c-f73d8f21b819",
   "metadata": {},
   "source": [
    "As you can see, context vectors are the same size as our inputs. In other ways, we simply modified the embeddings to reflect the attention to other tokens as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cf12ea-923a-407b-ad78-67ffb24fea88",
   "metadata": {},
   "source": [
    "# Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf525bb-ecce-445c-9fdf-6f69a20f4772",
   "metadata": {},
   "source": [
    "There are several ways to implement a self-attention layer. The original implementation  introduced in the original paper is called <em> \"Scaled Dot-Product Attention\" </em> as depicted in figure 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c644237b-9a7b-4f25-bcec-91700d1fb94d",
   "metadata": {},
   "source": [
    "Why <em> Scaled </em> dot-product? When scaling up the embedding dimension, which is typically greater than thousand for GPT-like LLMs, large dot products can result in very small gradients during backpropagation due to the softmax function applied to them. As dot products increase, the softmax function behaves more like a step function, resulting in gradients nearing zero. These small gradients can drastically slow down learning. In scaled dot-product attention, the dot products are scaled by the size of the embedding vectors so that we don't get too many large numbers during training. \n",
    "<center><figure><img src=\"imgs/scaled-dot-product.png\" alt=\"drawing\" width=\"300\"/><figcaption>Fig. 2: Scaled Dot-Product Attention.</figcaption></figure></center>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e477f498-1b6f-4eae-873a-539ab16cf96f",
   "metadata": {},
   "source": [
    "The scaled-dot-product is obtained using the following equation: \n",
    "\n",
    "$ (4) \\; Attention(Q,K,V) =  softmax(\\frac{QK^{T}}{\\sqrt{d_{k}}})V$   \n",
    "\n",
    "Where, Q, K, and V are called query, key, and value, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb83a651-879d-4506-88ba-3949a3289af4",
   "metadata": {},
   "source": [
    "# What are Query (Q), Key (K) and Value (V)? \n",
    "\n",
    "In attention mechanisms, we use terms \"key,\" \"query,\" and \"value\" which come from information retrieval and databases. They help us store, search, and get information efficiently.\n",
    "\n",
    "Think of a \"query\" like a search term you put into a database. It's what the model is currently focusing on or trying to understand, like a word in a sentence. The query helps the model figure out how much attention to give to other parts of the input.\n",
    "\n",
    "A \"key\" is like an index in a database used for searching. Each item in the input sequence, such as each word in a sentence, has a key. These keys are matched with the query to find relevant information.\n",
    "\n",
    "The \"value\" in this context is similar to the value in a key-value pair in a database. It represents the actual content or representation of the input items. Once the model determines which keys (and thus which parts of the input) are most relevant to the query (the current focus item), it retrieves the corresponding values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb355a1-e229-4655-8c9d-7763ecfd6d26",
   "metadata": {},
   "source": [
    "Let's see figure 2 in code. First, we write a function to implement a scaled dot-product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "671a0a1e-c96f-46f6-9d59-361998907582",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    dim_k = K.size(-1)\n",
    "    print(dim_k)\n",
    "    attn_scores = torch.matmul(Q, K.T)\n",
    "    attn_weights = torch.softmax(attn_scores / sqrt(dim_k),dim=-1)\n",
    "    return torch.matmul(attn_weights,V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbdd284-6b91-4cea-b3de-541469497f86",
   "metadata": {},
   "source": [
    "Then, we create self-attention class as follows. $W_{q}, W_{k}, W_{v}$ are weight matrices. We will use <code>nn.Linear</code> to take advantage of its weight initialization scheme and set <code>bias = False</code> to perform matrix multiplication as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e1fbc10-a6d8-4576-8a53-0a5d7344caa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "        self.W_k = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "        self.W_v = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys = self.W_k(x)\n",
    "        queries = self.W_q(x)\n",
    "        values = self.W_v(x)        \n",
    "  \n",
    "        context_vector = scaled_dot_product_attention(queries, keys, values)\n",
    "        return context_vector    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cf0696-88e1-494f-8389-d3a9e5427574",
   "metadata": {},
   "source": [
    "# Multi-head attention\n",
    "As we saw, the self-attention mechanism employs three independent linear transformations on each embedding to produce the query, key, and value vectors. Each projection has  its own set of trainable parameters, so that the model, especially the self-attention layer, can attend to various semantic features within the sequence and  learn to produce \"good\" context vectors.\t\t \t \t \t\t\n",
    "\t\t\t\n",
    "\n",
    "Additionally, it would be advantageous to incorporate multiple sets of linear projections, referred to as  “attention head”. But why several attention heads? The reason is the softmax function of a single head focuses on one similarity aspect. By employing several heads, the model simultaneously focuses on multiple aspects of similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcf92e1-045f-41b9-b25c-84144b1527b6",
   "metadata": {},
   "source": [
    "$(5) \\; MultiHead(Q,K,V) =  Concat(head_{1}, ..., head_{h})W^{O} $ \n",
    "\n",
    "where, \n",
    "\n",
    "$ head_{i} = Attention(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})$ and, \\\n",
    "$ W_{i}^{Q} \\in \\mathbb{R}^{d_{e}\\times d_{k}}, W_{i}^{K} \\in \\mathbb{R}^{d_{e} \\times d_{k}}, W_{i}^{V} \\in \\mathbb{R}^{d_{e} \\times d_{v}} and \\; W^{O} \\in \\mathbb{R}^{hd_{v} \\times d_{e}} $ are weight matrices.\n",
    "\n",
    "Also, $ d_{k} = d_{v} = d_{e} / h $ where, h is the number of heads\n",
    "\n",
    "These matrices transform input data into queries, keys, and values, which are crucial components of the attention mechanism. As the model is exposed to more data during training, it adjusts these trainable weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef2ec58-34f9-482c-8649-6468713539b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><figure><img src=\"imgs/multi-head-attention.png\" alt=\"drawing\" width=\"350\"/><figcaption>Fig. 3: Multi-head Attention.</figcaption></figure></center>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610edecb-de5b-4272-8044-e05fd52ad3c3",
   "metadata": {},
   "source": [
    "Now that we have implemented a self-attention mechanism, let's move forward and implement a multi-head attention mechanism according to figure 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6538be4-7a11-4264-8132-eb94d632aaf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size (d_e) is: 6, head dimension (d_h) is: 3 and number of heads are: 2\n"
     ]
    }
   ],
   "source": [
    "d_e = embeddings.shape[1]\n",
    "num_heads = 2\n",
    "d_h = d_e // num_heads\n",
    "print(f\"Embedding size (d_e) is: {d_e}, head dimension (d_h) is: {d_h} and number of heads are: {num_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e70d119e-df46-4856-9fa5-3b765ae41b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, head_dim):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SelfAttention(embed_size, head_dim) for _ in range(num_heads)])\n",
    "        self.output_linear = nn.Linear(embed_size, embed_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = torch.cat([head(inputs) for head in self.heads], dim=-1)\n",
    "        x = self.output_linear(x)\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc2f2f5-59ea-4f95-abc0-048945ed130a",
   "metadata": {},
   "source": [
    "Please note that the final linear layer is used to produce a tensor of the same size as our input tensor (i.e., 5x6 in our example). Let's check it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4595f4c7-7a73-4ab5-854e-b827bfc47ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "torch.Size([5, 6])\n"
     ]
    }
   ],
   "source": [
    "mult_head_attn = MultiHeadAttention(d_e,num_heads,d_h)\n",
    "attn_output = mult_head_attn(embeddings)\n",
    "print(attn_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3e862c-b98a-42a6-a970-568d2e512046",
   "metadata": {},
   "source": [
    "As you can see it generates an output tensor with the expected size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330892fb-a3c2-498d-912e-5109ab84abe8",
   "metadata": {},
   "source": [
    "If we look at the model architecture depicted in figure 3, there is a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "$(5) \\; FFN(x) = max(0, xW_{1} + b_{1})W_{2} + b_{2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf286d69-dc52-476a-91c8-a5a89dc0d19a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><figure><img src=\"imgs/transformer architecture.png\" alt=\"drawing\" width=\"400\"/><figcaption>Fig. 4: Transformer architecture.</figcaption></figure></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5ec97ccf-d64d-4914-a14d-63c50d22551b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ff_hidden_size = 4 * embed_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "323b6892-ac00-4f66-8a47-da9f0e86a998",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self, embed_size, ff_hidden_size):\n",
    "        super().__init__()\n",
    "        self.x1 = nn.Linear(embed_size, ff_hidden_size)\n",
    "        self.x2 = nn.Linear(ff_hidden_size, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.x1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.x2(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba336bba-6e31-4c6c-a129-60ca640e9bfd",
   "metadata": {},
   "source": [
    "Moreover, a residual connection is employed around each of the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is\n",
    "$ LayerNorm(x + Sublayer(x)) $, where $ Sublayer(x) $ is the function implemented by the sub-layer itself [1].\n",
    "\n",
    "Note: As a rule of thumb, the size of first layer is four times the size of the embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b1a91120-c000-429e-8e4c-0f263ee0cbf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_forward = FeedForwardLayer(embed_size, ff_hidden_size)\n",
    "ff_out = feed_forward(embeddings)\n",
    "ff_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "62260c30-6c51-41b5-9956-e2d4017ffeb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, head_dim, ff_hidden_size):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.multi_head_attn = MultiHeadAttention(embed_size,num_heads,head_dim)\n",
    "        self.feed_forward = FeedForwardLayer(embed_size,ff_hidden_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.norm1(x + self.multi_head_attn(x))\n",
    "        x = self.norm2( x + self.feed_forward(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e38cb9-859e-46d7-a47c-7d3b1c1f865e",
   "metadata": {},
   "source": [
    "Now, we can easily add the pieces together and build the encoder part as shown in the left side of figure 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c3ebd0b2-f160-4215-bfff-738d12eaa35d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(embed_size,num_heads,head_dim,ff_hidden_size)\n",
    "encoder_output = encoder(embeddings)\n",
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29bd1dd-c5d0-4de8-a5cc-2bec827abab8",
   "metadata": {},
   "source": [
    "In the next article, I will implement the decoder part. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185c47e1-4235-44b5-851a-e0b45587fc30",
   "metadata": {},
   "source": [
    "# References: \n",
    "\n",
    "[1] Ashish Vaswan, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin. \"Attention Is All You Need\". \tarXiv:1706.03762v7. \n",
    "\n",
    "[2] https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32457f41-99c2-409d-b138-ed6084075c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
