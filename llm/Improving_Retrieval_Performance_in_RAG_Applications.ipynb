{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"xuui3EgCLKzl"},"outputs":[],"source":["!pip install -U sentence-transformers transformers accelerate datasets huggingface_hub"]},{"cell_type":"code","source":["import torch\n","from sentence_transformers import SentenceTransformer\n","from sentence_transformers.evaluation import InformationRetrievalEvaluator\n","from sentence_transformers.losses import MultipleNegativesRankingLoss\n","from sentence_transformers.training_args import BatchSamplers\n","from datasets import load_dataset\n","from huggingface_hub import login\n","from datasets import load_dataset\n","from sentence_transformers import (\n","    SentenceTransformer,\n","    SentenceTransformerTrainer,\n","    SentenceTransformerTrainingArguments,\n","    SentenceTransformerModelCardData,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2xeQ-mgjMEGz","executionInfo":{"status":"ok","timestamp":1719543442246,"user_tz":240,"elapsed":10575,"user":{"displayName":"sepehr keykhaie","userId":"18194880490269398179"}},"outputId":"bdbecdb0-4cfc-462a-9757-4baea29cd846"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n","  from tqdm.autonotebook import tqdm, trange\n"]}]},{"cell_type":"markdown","metadata":{"id":"amPmyis8riYa"},"source":["# Improving Retrieval Performance in RAG Applications\n","\n","## Introduction\n","Embedding models have revolutionized the field of natural language processing (NLP). These models transform high-dimensional data (like text) into a lower-dimensional space while preserving relevant informational and relational properties. This transformation facilitates various tasks in natural language processing (NLP), including search, recommendation systems, and information retrieval.\n","\n","\n","### Retrieval-Augmented Generation (RAG)\n","\n","\n","Retrieval-Augmented Generation (RAG) combines the strengths of retrieval-based methods and generation-based methods to improve the performance of NLP systems. RAG applications retrieve relevant information from a large corpus of documents and use this information to generate more accurate and contextually appropriate responses. This approach has found applications in numerous domains, including chatbots, search engines, recommendation systems, and knowledge management systems.\n","\n","\n","The effectiveness of RAG applications heavily relies on the quality of the embedding models used for information retrieval. Embedding models must accurately capture the semantic meaning of queries and documents to ensure that the most relevant information is retrieved. However, the reliance on general-purpose embedding models often limits the performance of RAG applications in specific domains.\n","\n","\n","Embedding models are mostly trained on extensive corpuses of general knowledge, such as Wikipedia or Common Crawl, this broad approach can be limiting when applied to specialized domains. For example, models trained on general data may not perform well in technical domains without additional tuning. This limitation arises from the fact that general knowledge embeddings may not capture the nuances and specialized terminology unique to specific domains.\n","\n","\n","\n","\n","Customizing embedding models to capture domain-specific knowledge is crucial for enhancing the performance of RAG applications. Domain-specific embeddings are trained on specialized corpora that reflect the language and terminology used in a particular field. By doing so, these embeddings can better capture the semantic nuances and context-specific meanings that are essential for accurate information retrieval. This customization process involves training or fine-tuning models on specialized datasets, incorporating domain-specific vocabularies, and possibly adjusting model architectures to better handle the characteristics of the data.\n","\n","\n","\n","\n","### Boosting Retrieval Performance in RAG Applications\n","\n","\n","Enhancing retrieval performance is crucial for the success of RAG applications, as the quality of retrieved documents significantly impacts the quality of the generated content. Customizing embeddings can lead to more accurate and relevant data retrieval, which in turn improves the overall output of the RAG system. For instance, a RAG application in the medical field, trained with domain-specific embeddings, would be able to retrieve and generate more precise and clinically relevant information than one using a generic embedding model.\n","\n","\n","Sentence Transformers is a Python library for using and training embedding models for a wide range of applications, such as retrieval augmented generation, semantic search, semantic textual similarity, paraphrase mining, and more. Its v3.0 update is the largest since the project's inception, introducing a new training approach.\n","\n","\n","Developed by UKPLab, the Sentence Transformers library extends the popular BERT (Bidirectional Encoder Representations from Transformers) model by Hugging Face, but with a focus on producing better sentence-level embeddings. Unlike traditional BERT that outputs a high-dimensional vector for each token in the input text, Sentence Transformers generate a single fixed-size vector for the entire input sentence or paragraph, making them more practical for tasks that require sentence-level comparisons. With this library, we can utilize and train embedding models across different applications. These applications include RAG, semantic search, semantic textual similarity, and many others. The v3.0 update introduces a new trainer that makes it easier to fine-tune and train embedding models. This update includes enhanced components like diverse datasets, updated loss functions, and a streamlined training process, improving the efficiency and flexibility of model development. In this post, I'll show you how to finetune a sentence transformer model on a specific task using the Sentence Transformer library.  \n","\n","\n","## Training a Sentence Transformer\n","\n","Training Sentence Transformer models involves between 3 to 5 components.\n","\n","\n","\n","<center><figure><img src=\"../imgs/Sentence Transformer Training.png\" alt=\"drawing\" width=\"1100\"/><figcaption>Fig. 1: Sentence Transformer training components</figcaption></figure></center>\n","\n","### Dataset\n","\n","You can load your local dataset or Hugging Face Datasets using datasets.load_dataset(). One important consideration is that your dataset format should match your loss function. If your loss function requires a Label accordingly, then your dataset must have a column named “label” or “score”. All other columns are considered Inputs. The number of remaining columns must match the number of valid inputs for your chosen loss. The names of these columns are irrelevant, only the order matters. Table 1 shows the requirements for the loss functions in Sentence Transformers v3.0.\n","\n","For this demo, I use a dataset [SepKeyPro/trivia-anchor-positive-10k](https://huggingface.co/datasets/SepKeyPro/trivia-anchor-positive-10k) which includes (anchor, positive) pairs. We can load it using load_dataset().\n"]},{"cell_type":"code","source":["corpus_dataset = load_dataset(\"SepKeyPro/trivia-anchor-positive-10k\", split=\"train\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["83112689ebb14ae99562ce5cc6bb740f","560d554ce3ec427da30d3dfac591fccf","ddd371c29ef74047ba76626e4b99e32e","892e7baa79214b47a22440dd3a22d65c","d55fc55eb99c43e99c2386a5b39cf5bf","37aae39fc80543fda2a8661ea46f7b70","6ca0791dd16243ea98d64827cb6cd244","c3354fc9c0454ade96f9772d9babc66f","8a58e6d8d00b47d69d5dc3f83c8ff15c","455bb48264a84d31ae14760325218faf","bde78228263f4daaae703e4548998aee","9f4f912de34c423a9fe3416c18ad0069","10509c23b228481fb72c173f8b6874d9","eac46e0253224e3bb81ee7b15a409666","f4055711dfbb435197b050eb112571e1","8eb6b9b3d2f14a19a515ea442080bf24","cfa04b06e3da4645990cbc71cb9555af","dd354631ad7644c4a2cdfdaa7c6476e6","e55793a0ea054363a49c3ac628dcbe5f","26bcb0bb82f34333975df6680899de0d","706ca922da6e4e98ae45bf3c65889991","16152bbedb4641aeb98706cf2766c333","635c4b971ef44138b7f1f814662a8ea5","b3f711a1abb54c30b21aef930c826a6c","3e6ca5ad5f3d4a429e7856f88867cc9d","b70e8d228815414392344b717a3f3509","4811d30998664c62920779324ecc97e9","b93f1aec266945f483a384653669ea77","e5aebecd1499485fbc8a48a24a20a663","c105bba5b21747749324920f2dc076f3","ec534078d62448d7bff51f7d1966f7e2","cb728ab04a0549a1b0e0647139a32c9a","ec43300383464a6e93e67fa4f8a141ca"]},"id":"fLMt-8Oq28I3","executionInfo":{"status":"ok","timestamp":1719543461685,"user_tz":240,"elapsed":11433,"user":{"displayName":"sepehr keykhaie","userId":"18194880490269398179"}},"outputId":"190de2bf-9dbc-4209-e810-152e3026d8b3"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading readme:   0%|          | 0.00/347 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83112689ebb14ae99562ce5cc6bb740f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/19.9M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f4f912de34c423a9fe3416c18ad0069"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"635c4b971ef44138b7f1f814662a8ea5"}},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"kSoM0SMZLKzo"},"source":["### Loss Function\n","\n","The loss function is at the core of training in machine learning algorithms. Unfortunately, there is no single loss function that works best for all use cases. Choose your loss function based on your data, or curate your dataset based on your loss function. You can consult Table 1 for the loss function requirements."]},{"cell_type":"markdown","metadata":{"id":"B0doFWWpLKzo"},"source":["\n","<table border=\"1\">\n","    <caption>Table 1: Requirements for Loss Functions </caption>\n","    <tr>\n","        <th>Inputs</th>\n","        <th>Labels</th>\n","        <th>Appropriate Loss Functions</th>\n","    </tr>\n","    <tr>\n","        <td>single sentences</td>\n","        <td>class</td>\n","        <td>BatchAllTripletLoss<br> BatchHardSoftMarginTripletLoss<br> BatchHardTripletLoss<br> BatchSemiHardTripletLoss</td>\n","    </tr>\n","    <tr>\n","        <td>single sentences</td>\n","        <td>none</td>\n","        <td>ContrastiveTensionLoss<br> DenoisingAutoEncoderLoss</td>\n","    </tr>\n","    <tr>\n","        <td>(anchor, anchor) pairs</td>\n","        <td>none</td>\n","        <td>ContrastiveTensionLossInBatchNegatives</td>\n","    </tr>\n","    <tr>\n","        <td>(damaged_sentence, original_sentence) pairs</td>\n","        <td>none</td>\n","        <td>DenoisingAutoEncoderLoss</td>\n","    </tr>\n","    <tr>\n","        <td>(sentence_A, sentence_B) pairs</td>\n","        <td>class</td>\n","        <td>SoftmaxLoss</td>\n","    </tr>\n","    <tr>\n","        <td>(anchor, positive) pairs</td>\n","        <td>none</td>\n","        <td>CachedMultipleNegativesRankingLoss<br> MultipleNegativesRankingLoss<br> MultipleNegativesSymmetricRankingLoss<br> MegaBatchMarginLoss<br> CachedGISTEmbedLoss<br> GISTEmbedLoss</td>\n","    </tr>\n","    <tr>\n","        <td>(anchor, positive/negative) pairs</td>\n","        <td>1 if positive, 0 if negative</td>\n","        <td>ContrastiveLoss<br> OnlineContrastiveLoss</td>\n","    </tr>\n","    <tr>\n","        <td>(sentence_A, sentence_B) pairs</td>\n","        <td>float similarity score</td>\n","        <td>CoSENTLoss<br> AnglELoss<br> CosineSimilarityLoss</td>\n","    </tr>\n","    <tr>\n","        <td>(anchor, positive, negative) triplets</td>\n","        <td>none</td>\n","        <td>CachedMultipleNegativesRankingLoss<br> MultipleNegativesRankingLoss<br> TripletLoss<br>CachedGISTEmbedLoss<br>GISTEmbedLoss</td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","source":["Considering our dataset format and consulting Table 1, I select MultipleNegativesRankingLoss. The MultipleNegativesRankingLoss is a great loss function if we only have positive pairs as it adds in batch negative samples to the loss function to have per sample n-1 negative samples. For the model, I am going to use the BAAI/bge-base-en-v1.5 model, which is a pre-trained model on a large corpus of English text."],"metadata":{"id":"6jUqGDAoMqNm"}},{"cell_type":"code","source":["model_id = \"BAAI/bge-base-en-v1.5\"\n","model = SentenceTransformer(\n","    model_id,\n","    model_kwargs={\"attn_implementation\": \"sdpa\"},\n","    device=\"cuda\",\n","    model_card_data=SentenceTransformerModelCardData(\n","        language=\"en\",\n","        license=\"apache-2.0\",\n","        model_name=\"bge base trained on trivia anchor-positive\",\n","    )\n",")\n","\n","loss = MultipleNegativesRankingLoss(model)"],"metadata":{"id":"N4OFK0MU3Dc6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TGktxfBwLKzo"},"source":["### Training Arguments\n","\n","You can specify training parameters to improve training performance. Training Arguments are optional, however, you can experiment with them to see how they can improve your training performance. Table 2 shows some of the training arguments to look at.\n","\n","<table border=\"1\">\n","  <caption>Table 2: Training Arguments</caption>\n","  <tr>\n","    <th>Training Argument</th>\n","    <th>Explanation</th>\n","    <th>Data type</th>\n","  </tr>\n","  <tr>\n","    <td>learning_rate</td>\n","    <td>The learning rate of the optimizer.</td>\n","    <td>float</td>\n","  </tr>\n","  <tr>\n","    <td>lr_scheduler_type</td>\n","    <td>The scheduler type to use. Possible values are: “Constant”, “constant_with_warmup”, “cosine”, “cosine_with_warmup”, “linear_with_warmup”, “inverse_sqrt”. See <a href=\"https://huggingface.co/docs/transformers/main/en/main_classes/optimizer_schedules#transformers.SchedulerType\">SchedulerType<sup>1</sup></a> for more details.</td>\n","    <td>str</td>\n","  </tr>\n","  <tr>\n","    <td>warmup_ratio</td>\n","    <td>For schedulers with warmup, Ratio of total training steps used for a linear warmup from 0 to learning_rate.</td>\n","    <td>float</td>\n","  </tr>\n","  <tr>\n","    <td>num_train_epochs</td>\n","    <td>Total number of training epochs to perform.</td>\n","    <td>float</td>\n","  </tr>\n","  <tr>\n","    <td>max_steps</td>\n","    <td>If set to a positive number, the total number of training steps to perform. Overrides num_train_epochs.</td>\n","    <td>int</td>\n","  </tr>\n","  <tr>\n","    <td>per_device_train_batch_size</td>\n","    <td>The batch size per GPU core/CPU for training.</td>\n","    <td>int</td>\n","  </tr>\n","  <tr>\n","    <td>per_device_eval_batch_size</td>\n","    <td>The batch size per GPU core/CPU for evaluation.</td>\n","    <td>int</td>\n","  </tr>\n","  <tr>\n","    <td>auto_find_batch_size</td>\n","    <td>Whether to find a batch size that will fit into memory automatically through exponential decay, avoiding CUDA Out-of-Memory errors.</td>\n","    <td>bool</td>\n","  </tr>\n","  <tr>\n","    <td>fp16</td>\n","    <td>Whether to use fp16.</td>\n","    <td>bool</td>\n","  </tr>\n","  <tr>\n","    <td>bf16</td>\n","    <td>Whether to use bf16.</td>\n","    <td>bool</td>\n","  </tr>\n","  <tr>\n","    <td>gradient_accumulation_steps</td>\n","    <td>Number of updates steps to accumulate the gradients for, before performing a backward/update pass.</td>\n","    <td>int</td>\n","  </tr>\n","  <tr>\n","    <td>gradient_checkpointing</td>\n","    <td>If True, use gradient checkpointing to save memory at the expense of slower backward pass.</td>\n","    <td>bool</td>\n","  </tr>\n","  <tr>\n","    <td>eval_accumulation_steps</td>\n","    <td>Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU.</td>\n","    <td>int</td>\n","  </tr>\n","  <tr>\n","    <td>optim</td>\n","    <td>The optimizer to use. Some of the optimizers are: “adamw_hf”, “sgd”, “adamw_8bit”, “paged_adamw_32bit”, “paged_adamw_8bit”, “adagrad”, “rmsprop”, “rmsprop_bnb_32bit”. For the full list of optimizers available in Training Arguments see <a href=\"https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py\">Optimizers Names<sup>2</sup></a>.</td>\n","    <td>str</td>\n","  </tr>\n","  <tr>\n","    <td>eval_strategy</td>\n","    <td>The evaluation strategy to adopt during training. Possible values are: \"no\": No evaluation is done during training. \"steps\": Evaluation is done (and logged) every eval_steps. \"epoch\": Evaluation is done at the end of each epoch.</td>\n","    <td>str</td>\n","  </tr>\n","  <tr>\n","    <td>eval_steps</td>\n","    <td>Number of update steps between two evaluations if eval_strategy=\"steps\".</td>\n","    <td>int</td>\n","  </tr>\n","  <tr>\n","    <td>report_to</td>\n","    <td>The list of integrations to report the results and logs to. Possible values are: \"azure_ml\", \"codecarbon\", \"tensorboard\", \"wandb\".</td>\n","    <td>str</td>\n","  </tr>\n","</table>\n","\n","*Check [Training Arguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) on Hugging Face <img src=\"../imgs/hf-logo.svg\" alt=\"drawing\" width=\"25\"/> for a complete list of training arguments.  "]},{"cell_type":"code","source":["training_args = SentenceTransformerTrainingArguments(\n","    output_dir=\"models/bge-base-en-trivia\",\n","    num_train_epochs=1,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    gradient_accumulation_steps=4,\n","    warmup_ratio=0.1,\n","    learning_rate=2e-5,\n","    lr_scheduler_type=\"cosine\",\n","    optim=\"adamw_torch_fused\",\n","    tf32=True,\n","    bf16=True,\n","    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    logging_steps=10,\n","    save_total_limit=2,  # save only the last 2 models\n","    run_name=\"bge-base-en-trivia\",  # Will be used in W&B if `wandb` is installed\n",")"],"metadata":{"id":"t6KBFxDY3r_d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vrwMz8CzLKzo"},"source":["### Evaluator\n","An Evaluator can be used to assess the model’s performance with useful metrics before, during, or after training. They evaluate the model based on the eval_strategy and eval_steps Training Arguments. Table 3 shows some of Evaluators available in Sentence Tranformers."]},{"cell_type":"markdown","metadata":{"id":"tX9SNRUbLKzo"},"source":["<table border=\"1\">\n","  <caption>Table 3: Evaluators in Sentence Tranformers</caption>\n","  <tr>\n","    <th>Evaluator</th>\n","    <th>Required Data</th>\n","  </tr>\n","  <tr>\n","    <td>BinaryClassificationEvaluator</td>\n","    <td>Pairs with class labels</td>\n","  </tr>\n","  <tr>\n","    <td>EmbeddingSimilarityEvaluator</td>\n","    <td>Pairs with similarity scores</td>\n","  </tr>\n","  <tr>\n","    <td>InformationRetrievalEvaluator</td>\n","    <td>Queries (qid => question), Corpus (cid => document), and relevant documents (qid => set[cid])</td>\n","  </tr>\n","  <tr>\n","    <td>ParaphraseMiningEvaluator</td>\n","    <td>Mapping of IDs to sentences & pairs with IDs of duplicate sentences.</td>\n","  </tr>\n","  <tr>\n","    <td>RerankingEvaluator</td>\n","    <td>List of {'query': '...', 'positive': [...], 'negative': [...]} dictionaries.</td>\n","  </tr>\n","  <tr>\n","    <td>TripletEvaluator</td>\n","    <td>(anchor, positive, negative) pairs.</td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","source":["Since our focus is on improving information retrieval performance, we can choose the InformationRetrievalEvaluator as our evaluator. Referring to the required data in Table 3, we will need three dictionaries for this evaluator: 1) a queries dictionary, which includes query IDs and the corresponding queries; 2) a corpus dictionary, which contains corpus IDs and the corresponding documents; and 3) a dictionary of query IDs and corpus IDs for relevant documents related to each query. To generate the queries dictionary, we can use (id, anchor) pairs from our test dataset. (id, positive) pairs from the whole dataset can be used as our corpus dictionary, and (id, positive) pairs from the test dataset can be used as our relevant documents dictionary. This code snippet shows the process:"],"metadata":{"id":"hmVDF5J5xMb7"}},{"cell_type":"code","source":["corpus_dataset = load_dataset(\"SepKeyPro/trivia-anchor-positive-10k\", split=\"train\")\n","\n","split_dataset = corpus_dataset.train_test_split(test_size=0.1)\n","train_dataset = split_dataset[\"train\"]\n","test_dataset = split_dataset[\"test\"]\n","\n","# Convert the datasets to dictionaries\n","corpus = dict(\n","    zip(corpus_dataset[\"id\"], corpus_dataset[\"positive\"])\n",")\n","queries = dict(\n","    zip(test_dataset[\"id\"], test_dataset[\"anchor\"])\n",")\n","relevant_docs = {}\n","for q_id in queries:\n","  relevant_docs[q_id] = [q_id]\n","\n","relevant_docs = {key: torch.tensor(value).to(torch.device('cuda')) for key, value in relevant_docs.items()}\n"],"metadata":{"id":"egumy_9N1Nj3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ir_evaluator = InformationRetrievalEvaluator(\n","    queries=queries,\n","    corpus=corpus,\n","    relevant_docs=relevant_docs,\n","    name=\"trivia-anchor-positive-dev\",\n",")\n","\n","ir_evaluator(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iakYmK9VyWAB","executionInfo":{"status":"ok","timestamp":1719547096434,"user_tz":240,"elapsed":80852,"user":{"displayName":"sepehr keykhaie","userId":"18194880490269398179"}},"outputId":"27d0b585-835c-4ea3-bfec-b4993414429f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'trivia-anchor-positive-dev_cosine_accuracy@1': 0.686,\n"," 'trivia-anchor-positive-dev_cosine_accuracy@3': 0.857,\n"," 'trivia-anchor-positive-dev_cosine_accuracy@5': 0.9,\n"," 'trivia-anchor-positive-dev_cosine_accuracy@10': 0.93,\n"," 'trivia-anchor-positive-dev_cosine_precision@1': 0.686,\n"," 'trivia-anchor-positive-dev_cosine_precision@3': 0.2856666666666666,\n"," 'trivia-anchor-positive-dev_cosine_precision@5': 0.18,\n"," 'trivia-anchor-positive-dev_cosine_precision@10': 0.09300000000000001,\n"," 'trivia-anchor-positive-dev_cosine_recall@1': 0.686,\n"," 'trivia-anchor-positive-dev_cosine_recall@3': 0.857,\n"," 'trivia-anchor-positive-dev_cosine_recall@5': 0.9,\n"," 'trivia-anchor-positive-dev_cosine_recall@10': 0.93,\n"," 'trivia-anchor-positive-dev_cosine_ndcg@10': 0.8160966554498293,\n"," 'trivia-anchor-positive-dev_cosine_mrr@10': 0.7786615079365087,\n"," 'trivia-anchor-positive-dev_cosine_map@100': 0.7808572437466846,\n"," 'trivia-anchor-positive-dev_dot_accuracy@1': 0.686,\n"," 'trivia-anchor-positive-dev_dot_accuracy@3': 0.857,\n"," 'trivia-anchor-positive-dev_dot_accuracy@5': 0.9,\n"," 'trivia-anchor-positive-dev_dot_accuracy@10': 0.93,\n"," 'trivia-anchor-positive-dev_dot_precision@1': 0.686,\n"," 'trivia-anchor-positive-dev_dot_precision@3': 0.2856666666666666,\n"," 'trivia-anchor-positive-dev_dot_precision@5': 0.18,\n"," 'trivia-anchor-positive-dev_dot_precision@10': 0.09300000000000001,\n"," 'trivia-anchor-positive-dev_dot_recall@1': 0.686,\n"," 'trivia-anchor-positive-dev_dot_recall@3': 0.857,\n"," 'trivia-anchor-positive-dev_dot_recall@5': 0.9,\n"," 'trivia-anchor-positive-dev_dot_recall@10': 0.93,\n"," 'trivia-anchor-positive-dev_dot_ndcg@10': 0.8160966554498293,\n"," 'trivia-anchor-positive-dev_dot_mrr@10': 0.7786615079365087,\n"," 'trivia-anchor-positive-dev_dot_map@100': 0.7808572437466846}"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"FFL0QQ5iLKzp"},"source":["### Trainer\n","The Trainer is where all previous components come together. We only have to specify the trainer with the model, training arguments (optional), training dataset, evaluation dataset (optional), loss function, evaluator (optional) and we can start training."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":267},"id":"NFyBsIERLKzp","executionInfo":{"status":"ok","timestamp":1719548198444,"user_tz":240,"elapsed":201241,"user":{"displayName":"sepehr keykhaie","userId":"18194880490269398179"}},"outputId":"c3050917-81bc-480c-ae39-893431dc19e8"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [140/140 03:18, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Trivia-anchor-positive-dev Cosine Accuracy@1</th>\n","      <th>Trivia-anchor-positive-dev Cosine Accuracy@3</th>\n","      <th>Trivia-anchor-positive-dev Cosine Accuracy@5</th>\n","      <th>Trivia-anchor-positive-dev Cosine Accuracy@10</th>\n","      <th>Trivia-anchor-positive-dev Cosine Precision@1</th>\n","      <th>Trivia-anchor-positive-dev Cosine Precision@3</th>\n","      <th>Trivia-anchor-positive-dev Cosine Precision@5</th>\n","      <th>Trivia-anchor-positive-dev Cosine Precision@10</th>\n","      <th>Trivia-anchor-positive-dev Cosine Recall@1</th>\n","      <th>Trivia-anchor-positive-dev Cosine Recall@3</th>\n","      <th>Trivia-anchor-positive-dev Cosine Recall@5</th>\n","      <th>Trivia-anchor-positive-dev Cosine Recall@10</th>\n","      <th>Trivia-anchor-positive-dev Cosine Ndcg@10</th>\n","      <th>Trivia-anchor-positive-dev Cosine Mrr@10</th>\n","      <th>Trivia-anchor-positive-dev Cosine Map@100</th>\n","      <th>Trivia-anchor-positive-dev Dot Accuracy@1</th>\n","      <th>Trivia-anchor-positive-dev Dot Accuracy@3</th>\n","      <th>Trivia-anchor-positive-dev Dot Accuracy@5</th>\n","      <th>Trivia-anchor-positive-dev Dot Accuracy@10</th>\n","      <th>Trivia-anchor-positive-dev Dot Precision@1</th>\n","      <th>Trivia-anchor-positive-dev Dot Precision@3</th>\n","      <th>Trivia-anchor-positive-dev Dot Precision@5</th>\n","      <th>Trivia-anchor-positive-dev Dot Precision@10</th>\n","      <th>Trivia-anchor-positive-dev Dot Recall@1</th>\n","      <th>Trivia-anchor-positive-dev Dot Recall@3</th>\n","      <th>Trivia-anchor-positive-dev Dot Recall@5</th>\n","      <th>Trivia-anchor-positive-dev Dot Recall@10</th>\n","      <th>Trivia-anchor-positive-dev Dot Ndcg@10</th>\n","      <th>Trivia-anchor-positive-dev Dot Mrr@10</th>\n","      <th>Trivia-anchor-positive-dev Dot Map@100</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.045100</td>\n","      <td>No log</td>\n","      <td>0.672000</td>\n","      <td>0.842000</td>\n","      <td>0.877000</td>\n","      <td>0.914000</td>\n","      <td>0.672000</td>\n","      <td>0.280667</td>\n","      <td>0.175400</td>\n","      <td>0.091400</td>\n","      <td>0.672000</td>\n","      <td>0.842000</td>\n","      <td>0.877000</td>\n","      <td>0.914000</td>\n","      <td>0.800503</td>\n","      <td>0.763353</td>\n","      <td>0.766189</td>\n","      <td>0.672000</td>\n","      <td>0.842000</td>\n","      <td>0.877000</td>\n","      <td>0.914000</td>\n","      <td>0.672000</td>\n","      <td>0.280667</td>\n","      <td>0.175400</td>\n","      <td>0.091400</td>\n","      <td>0.672000</td>\n","      <td>0.842000</td>\n","      <td>0.877000</td>\n","      <td>0.914000</td>\n","      <td>0.800503</td>\n","      <td>0.763353</td>\n","      <td>0.766189</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=140, training_loss=0.012902945463013436, metrics={'train_runtime': 199.3553, 'train_samples_per_second': 45.146, 'train_steps_per_second': 0.702, 'total_flos': 0.0, 'train_loss': 0.012902945463013436, 'epoch': 0.9946714031971581})"]},"metadata":{},"execution_count":33}],"source":["trainer = SentenceTransformerTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset.select_columns([\"anchor\",\"positive\"]),\n","    loss=loss,\n","    evaluator=ir_evaluator,\n",")\n","trainer.train()"]},{"cell_type":"code","source":["model.save_pretrained(\"models/bge-base-en-trivia/final\")"],"metadata":{"id":"ueiEegVLDEto"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.push_to_hub(\"bge-base-en-trivia-anchor-positive\", token=\"Your Token\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["ebf7786115b143f887c6e45b08bc49ea","d141450add8e4c4892c767c9bf358224","c3924ce22e984b11a1a296a4263925a4","05cd00fe998041feb8ebc2914fad6ebf","927d1b73c3ba4292862472a26498ae7b","01c6940a481e42ba978e1b185ba773de","69d65fd5f9344dc9a3b2309c62334621","f499283d7c2a42e88afad15ffe9476da","42e73848b11f4452a300565ef468d748","7a599ce27ea145219d8cbe3cb2d1f4d1","48e176a5d4374e6e88677ba31282a2d9"]},"id":"G6z0gBVxEOfd","executionInfo":{"status":"ok","timestamp":1719548521922,"user_tz":240,"elapsed":52661,"user":{"displayName":"sepehr keykhaie","userId":"18194880490269398179"}},"outputId":"aa10e796-0632-43dd-aa28-efe2de1ea082"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebf7786115b143f887c6e45b08bc49ea"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["'https://huggingface.co/SepKeyPro/bge-base-en-trivia-anchor-positive/commit/316293dc07e19505049fd6acef5d71abd065c9f0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":35}]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"83112689ebb14ae99562ce5cc6bb740f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_560d554ce3ec427da30d3dfac591fccf","IPY_MODEL_ddd371c29ef74047ba76626e4b99e32e","IPY_MODEL_892e7baa79214b47a22440dd3a22d65c"],"layout":"IPY_MODEL_d55fc55eb99c43e99c2386a5b39cf5bf"}},"560d554ce3ec427da30d3dfac591fccf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_37aae39fc80543fda2a8661ea46f7b70","placeholder":"​","style":"IPY_MODEL_6ca0791dd16243ea98d64827cb6cd244","value":"Downloading readme: 100%"}},"ddd371c29ef74047ba76626e4b99e32e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3354fc9c0454ade96f9772d9babc66f","max":347,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8a58e6d8d00b47d69d5dc3f83c8ff15c","value":347}},"892e7baa79214b47a22440dd3a22d65c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_455bb48264a84d31ae14760325218faf","placeholder":"​","style":"IPY_MODEL_bde78228263f4daaae703e4548998aee","value":" 347/347 [00:00&lt;00:00, 27.3kB/s]"}},"d55fc55eb99c43e99c2386a5b39cf5bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37aae39fc80543fda2a8661ea46f7b70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ca0791dd16243ea98d64827cb6cd244":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3354fc9c0454ade96f9772d9babc66f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a58e6d8d00b47d69d5dc3f83c8ff15c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"455bb48264a84d31ae14760325218faf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bde78228263f4daaae703e4548998aee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f4f912de34c423a9fe3416c18ad0069":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_10509c23b228481fb72c173f8b6874d9","IPY_MODEL_eac46e0253224e3bb81ee7b15a409666","IPY_MODEL_f4055711dfbb435197b050eb112571e1"],"layout":"IPY_MODEL_8eb6b9b3d2f14a19a515ea442080bf24"}},"10509c23b228481fb72c173f8b6874d9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfa04b06e3da4645990cbc71cb9555af","placeholder":"​","style":"IPY_MODEL_dd354631ad7644c4a2cdfdaa7c6476e6","value":"Downloading data: 100%"}},"eac46e0253224e3bb81ee7b15a409666":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e55793a0ea054363a49c3ac628dcbe5f","max":19887098,"min":0,"orientation":"horizontal","style":"IPY_MODEL_26bcb0bb82f34333975df6680899de0d","value":19887098}},"f4055711dfbb435197b050eb112571e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_706ca922da6e4e98ae45bf3c65889991","placeholder":"​","style":"IPY_MODEL_16152bbedb4641aeb98706cf2766c333","value":" 19.9M/19.9M [00:05&lt;00:00, 3.68MB/s]"}},"8eb6b9b3d2f14a19a515ea442080bf24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfa04b06e3da4645990cbc71cb9555af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd354631ad7644c4a2cdfdaa7c6476e6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e55793a0ea054363a49c3ac628dcbe5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26bcb0bb82f34333975df6680899de0d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"706ca922da6e4e98ae45bf3c65889991":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16152bbedb4641aeb98706cf2766c333":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"635c4b971ef44138b7f1f814662a8ea5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b3f711a1abb54c30b21aef930c826a6c","IPY_MODEL_3e6ca5ad5f3d4a429e7856f88867cc9d","IPY_MODEL_b70e8d228815414392344b717a3f3509"],"layout":"IPY_MODEL_4811d30998664c62920779324ecc97e9"}},"b3f711a1abb54c30b21aef930c826a6c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b93f1aec266945f483a384653669ea77","placeholder":"​","style":"IPY_MODEL_e5aebecd1499485fbc8a48a24a20a663","value":"Generating train split: 100%"}},"3e6ca5ad5f3d4a429e7856f88867cc9d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c105bba5b21747749324920f2dc076f3","max":10000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ec534078d62448d7bff51f7d1966f7e2","value":10000}},"b70e8d228815414392344b717a3f3509":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb728ab04a0549a1b0e0647139a32c9a","placeholder":"​","style":"IPY_MODEL_ec43300383464a6e93e67fa4f8a141ca","value":" 10000/10000 [00:00&lt;00:00, 58259.42 examples/s]"}},"4811d30998664c62920779324ecc97e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b93f1aec266945f483a384653669ea77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5aebecd1499485fbc8a48a24a20a663":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c105bba5b21747749324920f2dc076f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec534078d62448d7bff51f7d1966f7e2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cb728ab04a0549a1b0e0647139a32c9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec43300383464a6e93e67fa4f8a141ca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ebf7786115b143f887c6e45b08bc49ea":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d141450add8e4c4892c767c9bf358224","IPY_MODEL_c3924ce22e984b11a1a296a4263925a4","IPY_MODEL_05cd00fe998041feb8ebc2914fad6ebf"],"layout":"IPY_MODEL_927d1b73c3ba4292862472a26498ae7b"}},"d141450add8e4c4892c767c9bf358224":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_01c6940a481e42ba978e1b185ba773de","placeholder":"​","style":"IPY_MODEL_69d65fd5f9344dc9a3b2309c62334621","value":"model.safetensors: 100%"}},"c3924ce22e984b11a1a296a4263925a4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f499283d7c2a42e88afad15ffe9476da","max":437951328,"min":0,"orientation":"horizontal","style":"IPY_MODEL_42e73848b11f4452a300565ef468d748","value":437951328}},"05cd00fe998041feb8ebc2914fad6ebf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a599ce27ea145219d8cbe3cb2d1f4d1","placeholder":"​","style":"IPY_MODEL_48e176a5d4374e6e88677ba31282a2d9","value":" 438M/438M [00:45&lt;00:00, 15.1MB/s]"}},"927d1b73c3ba4292862472a26498ae7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01c6940a481e42ba978e1b185ba773de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69d65fd5f9344dc9a3b2309c62334621":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f499283d7c2a42e88afad15ffe9476da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42e73848b11f4452a300565ef468d748":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7a599ce27ea145219d8cbe3cb2d1f4d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48e176a5d4374e6e88677ba31282a2d9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}