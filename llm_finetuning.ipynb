{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc00f3d3-1efa-4944-badc-d1d992685f9c",
   "metadata": {},
   "source": [
    "# Different LLM Fine-Tuning Techniques\n",
    "\n",
    "In this article, I will talk about the different fine-tuning techniques for LLMs. I will describe each technique and then show them in action using Hugging Face <img src=\"imgs/hf-logo.svg\" alt=\"drawing\" width=\"30\"/> libraries. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aea827-af87-40bc-b527-83ed30646fd6",
   "metadata": {},
   "source": [
    "# Fine-tuning vs. Pre-training\n",
    "\n",
    "## Pre-training\n",
    "\n",
    "The main goal of pre-training is to develop a general understanding of language across a broad spectrum. This foundational knowledge is crucial for the model to perform a wide range of language tasks. Pre-training involves a massive and diverse corpus that includes books, articles, websites, and other forms of written text from a wide array of subjects and domains. The model is trained using <em>unsupervised</em> learning techniques, primarily predicting the next word in a sequence. This extensive training helps the model understand context, grammar, and a vast vocabulary. However, pre-training requires significant computational power and time, often involving hundreds of GPUs or TPUs running for weeks or months due to the big dataset and the complexity of the model. As a result, the model is highly generalist, capable of performing reasonably well across a wide range of tasks without further modification.\n",
    "\n",
    "## Fine-tuning\n",
    "\n",
    "Fine-tuning tailors the pre-trained model to specific tasks or domains. The aim is to optimize the model's performance for particular applications, such as legal document analysis, medical diagnosis, or customer service interactions. Fine-tuning needs more specific and limited data compared to pre-training. It focuses on the particular language, style, or content relevant to the task or domain at hand. During fine-tuning, the model continues to learn, but this time under <em>supervised</em> conditions where it adjusts its parameters based on feedback specific to the task (e.g., correct answers in a Q&A task). Unlike pre-training, fine-tuning requires fewer computational resources. Itâ€™s usually quicker because the model is already largely trained and only needs adjustments to its existing knowledge base. The resulting model is expert in its specific area and may lose some of its general applicability outside the targeted domain. \n",
    "\n",
    "As we mentioned above, fine-tuning is conducted in supervised manner. Let's delve deeper in Supervised Fine Tuning.  \n",
    "\n",
    "\n",
    "## Supervised Fine-tuning (SFT)\n",
    "\n",
    "This technique is called \"supervised\" because the new dataset used for fine-tuning is labeled, meaning that the correct answers (or labels) are provided for each input example. The fine-tuning process uses this labeled data to adjust the model's weights to better predict the correct labels for new inputs. Typically, a lower learning rate is used during fine-tuning compared to initial training. This is to make smaller adjustments to the weights of the model, because it makes the fine-tuning process more stable and ensures the model retains the previously learned features without drastic alterations. \n",
    "\n",
    "SFT trains models to follow the instructions given in the labeled dataset, which enables them to perform well on the new task . However, these models might still produce harmful or unethical outputs despite their capacity to follow instructions. To better align these models with human values, further training with pairwise preference data is essential, employing methods like reinforcement learning with human feedback (RLHF) and direct preference optimization (DPO). Figure 1 shows the LLM training process. \n",
    "\n",
    "\n",
    "<center><figure><img src=\"imgs/OPRO.png\" alt=\"drawing\" width=\"1000\"/><figcaption>Fig. 1: Language Model Training</figcaption></figure></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecfba0b-7616-4cb6-87db-b58fa5d8cb2d",
   "metadata": {},
   "source": [
    "# Preference Alignment without Reinforcement Learning\n",
    "\n",
    "Working with RLHF and developing a reward model has always been a challenging task, since it requires extensive hyperparameter searching due to the instability of PPO and the sensitivity of the reward models. For this reason, researchers recently have proposed several preference alignment approaches that eliminate the use of reward models and directly use preferred and rejected responses to align the model towards the better responses. In this section, I will talk about two recent techniques namely DPO and ORPO. \n",
    "\n",
    "## Direct Preference Optimization (DPO)\n",
    "\n",
    "DPO fine-tunes a language model to align with human preferences without explicit use of a reward model or reinforcement learning. DPO achieves the same goals as current RLHF algorithms but is easier to implement and train. DPO update increases the relative log probability of preferred to dispreferred responses, but it incorporates a dynamic, per-example importance weight that prevents the model degeneration.\n",
    "\n",
    "### In a simple way, DPO works as follow:\n",
    "\n",
    "\n",
    "At the beginning of the fine-tuning, a reference policy (model) $ \\pi_{ref}$ is generated, by freezing the trainable parameters of the initial model fin-tuned in a supervised manner (i.e., SFT) \n",
    "\n",
    "A dataset with tuples of $ (x, y_{w}, y_{l}) $ is generated, where $ x $ is the prompt, and $ y_{w} $ is the preferred (chosen) completion and $ y_{l} $ is the dispreferred (rejected) completion (see listing 1 for a toy dataset).\n",
    "\n",
    "For each prompt in the dataset, both the refernce policy and language model policy, $ \\pi_{\\theta} $ (i.e., the new trainable model),  score the chosen and rejected responses that are used to in the DPO loss function:\n",
    "\n",
    "$ L_{DPO}(\\pi_{ref},\\pi_{\\theta}) = - \\mathbb{E_{(x,y_{w},y_{i})}} \\; [log \\; \\sigma( \\beta \\; log \\frac {\\pi_{\\theta}(y_{w} | x)}{\\pi_{ref}(y_{w} | x)} - \\beta \\; log \\frac {\\pi_{\\theta}(y_{l} | x)}{\\pi_{ref}(y_{l} | x)} )]$\n",
    "\n",
    "In this equation, $ \\sigma $ is sigmoid function and $ \\beta $ is temperature parameter (mostly in the range 0.1 to 0.5). This controls how much we pay attention to the reference model. \n",
    "\n",
    "\n",
    "Intuitively, the gradient of the loss function $L_{DPO}$ increases the likelihood of the preferred completions $y_{w}$ and decreases the likelihood of dispreferred completions\n",
    "$y_{l}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e59421e8-0804-4624-9ac1-58f52ddef266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dpo_dataset_dict = {\n",
    "    \"prompt\": [\n",
    "        \"hello\",\n",
    "        \"how are you\",\n",
    "        \"What is your name?\",\n",
    "        \"What is your name?\",\n",
    "        \"Which is the best programming language?\",\n",
    "        \"Which is the best programming language?\",\n",
    "        \"Which is the best programming language?\",\n",
    "    ],\n",
    "    \"chosen\": [\n",
    "        \"hi nice to meet you\",\n",
    "        \"I am fine\",\n",
    "        \"My name is Mary\",\n",
    "        \"My name is Mary\",\n",
    "        \"Python\",\n",
    "        \"Python\",\n",
    "        \"Java\",\n",
    "    ],\n",
    "    \"rejected\": [\n",
    "        \"leave me alone\",\n",
    "        \"I am not fine\",\n",
    "        \"Whats it to you?\",\n",
    "        \"I dont have a name\",\n",
    "        \"Javascript\",\n",
    "        \"C++\",\n",
    "        \"C++\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae658b6e-11eb-4187-9246-924721a2a3fa",
   "metadata": {},
   "source": [
    "Listing 1: an example of ORPO-DPO dataset [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b845d08b-091d-4a8e-97b1-cb128c5867b1",
   "metadata": {},
   "source": [
    "# Odds Ratio Preference Optimization (ORPO)\n",
    "\n",
    "DPO techniques sometimes degrades the quality of the model. Recently, researchers have proposed odds ratio preference optimization (ORPO) technique which combines the supervised fine-tuning with a preference alignment technique, which efficiently penalizes the model from learning undesired generation styles during SFT. \n",
    "\n",
    "Unlike DPO that uses a reference policy (model) to align the new model, ORPO do so without a reference model in a single-step manner by assigning a weak penalty to the rejected responses and a strong reward to the chosen responses with a simple log odds ratio term added to the negative log-likelihood (NLL) loss of SFT.\n",
    "\n",
    "SFT plays a significant role in tailoring the pre-trained language models to the desired domain by increasing the log probabilities of pertinent tokens. However, this also increases the likelihood of generating undesirable tokens. ORPO reduces the generation of unwanted answers by adding an odds ratio-based penalty to the conventional negative log-likelihood (NLL):\n",
    "\n",
    "$ L_{ORPO} = \\mathbb{E_{(x,y_{w},y_{i})}} \\; [L_{SFT} + \\lambda.L_{OR}]$\n",
    "\n",
    "$L_{SFT}$ is negative log-likelihood loss function to maximize the likelihood of generating the reference tokens. $L_{OR}$ is given by:\n",
    "\n",
    "$L_{OR} = -log \\; \\sigma( log \\frac {odds_{\\theta}(y_{w} | x)}{odds_{\\theta}(y_{l} | x)})$\n",
    "\n",
    "where, $ odds(y|x) $ simply indicates how much more likely it is for the model to generate $y$ than not generating it. \n",
    "\n",
    "In the above, $L_{OR}$ maximizes the odds ratio between the likelihood of generating the favored response $y_{w}$ and the disfavored response $y_{l}$. Log sigmoid function $ log \\; \\sigma $ converts it to a minimization problem to be added to the original loss function, and $\\lambda$ is a weight parameter.\n",
    "\n",
    "In general $ L_{ORPO} $ aligns the language model to adapt to the specific subset of the desired domain and disfavor generations in the rejected response sets.\n",
    "\n",
    "SFT uses NLL to penalize the model if it doesn't predict the reference answers well. However, it only focuses on generation of chosen responses and there is no mechanism to penalize generation of rejected responses. \n",
    "\n",
    "ORPO expect datasets in a same format as DPO shown in listing 1.\n",
    "\n",
    "Figure 2, compares ORPO with DPO\n",
    "\n",
    "<center><figure><img src=\"imgs/ORPO-DPO.png\" alt=\"drawing\" width=\"600\"/><figcaption>Fig. 2: DPO vs. ORPO</figcaption></figure></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d47a37-292a-49b2-a586-aaaa3f1fd22b",
   "metadata": {},
   "source": [
    "Now that we have a basic understanding about these fine-tuning techniques, let's enjoy seeing them in action. In this section, I will demonstrate how to fine-tune a new Llama 3 model, using the aforementioned techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5610112-58be-4a9e-92d5-c584630167a4",
   "metadata": {},
   "source": [
    "# Fine-tuning Llama 3\n",
    "\n",
    "First and foremest, we should prepare our preference dataset in ORPO/DPO compatible format. The entries should be named:\n",
    "\n",
    "* prompt\n",
    "* chosen\n",
    "* rejected\n",
    "\n",
    "Moreover, they need to follow the chat template of the underlying language model. A good prefernce dataset with more than 40k samples is available on Hugging Face hub [orpo-dpo-mix-40k] (https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k). <code>apply_chat_template()</code> is part of the tokenizer. It converts conversations, represented as lists of messages, into a single tokenizable string in the format that the model expects. Let see it in action for llama3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28788c5d-362b-4de2-b5b2-d60ba01ff063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "chat = [\n",
    "   {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "   {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "]\n",
    "tokenizer.apply_chat_template(chat, tokenize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd8529a-b822-446b-8218-ca122f380e57",
   "metadata": {},
   "source": [
    "<|im_start|>user\n",
    "Hello, how is the weather today?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "It's currently cloudy and 55.4 F?<|im_end|>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e78ca17-8344-4a82-886c-da16b0a70cc4",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "\n",
    "We can reduce the memory and computation cost by applying a quantization to the model that will represent weights and activations in a lower precision format such as 8-bit integers.<code>BitsAndBytes</code> library for this purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f83b3f-a731-4301-beb1-62a7d11494e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, #1\n",
    "    bnb_4bit_quant_type=\"nf4\", #2\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, #3\n",
    "    bnb_4bit_use_double_quant=True, #4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ecad84-1816-4f2e-a890-fc66b3b3f952",
   "metadata": {},
   "source": [
    "Here, #1 loads the model in 4-bit quantization, #2 says to use NormalFloat4 format which is a 4-bit quantization used in QLoRA. #3 will use torch.bfloat16 as data type for computation which can improve computation speed in some cases such as matrix operations. Finally, #4 will apply a nested quantization technique for better memory efficiency without sacrificing performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b8badb-837c-4106-bc2f-8a42712223e9",
   "metadata": {},
   "source": [
    "We also need to leverage LoRAConfig to train the 4-bit model which will be used in the trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8281f0a-51ca-46a6-9371-7964a6225f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477e4cf4-278a-405c-9dec-f561a59efe0b",
   "metadata": {},
   "source": [
    "## Fine-tuning using DPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13caacd4-e24a-4230-85e6-8b018d4d93e1",
   "metadata": {},
   "source": [
    "As we discussed above, DPO needs a curated dataset with \"chosen\" and \"rejected\" responses to a \"prompt\". Hugging Face has more than 50 DPO-compatible datasets [here](https://huggingface.co/datasets?other=dpo). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762cc642-45aa-403a-9c67-4ebd3f21a2b5",
   "metadata": {},
   "source": [
    "## Fine-tuning using ORPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffd2f72-5408-4b59-9c7f-3b00a511db85",
   "metadata": {},
   "source": [
    "Hugging Face TRL library supports ORPO fine-tuning using the ORPO Trainer <code>ORPOTrainer()</code> for training language models from a preference dataset. \n",
    "\n",
    "First, we set the hyperparameters along with the other configuration using <code>ORPOConfig()</code>. Here are a set of hyperparameters suggested in the original paper []."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dced06d-68b6-4977-91ac-3101552a1b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "orpo_config = ORPOConfig(\n",
    "    learning_rate=8e-6, #1\n",
    "    beta=0.1,# lambda in the original paper\n",
    "    max_length=1024,#3\n",
    "    max_prompt_length=512,#4\n",
    "    optim=\"paged_adamw_8bit\",#5\n",
    "    num_train_epochs=1,#6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ea2bf1-30a7-45b0-ab4e-47fa85dfd512",
   "metadata": {},
   "source": [
    "Then, we define a trainer and train our mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed28eb-94e4-4f06-9e8c-ef12c59a70d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ORPOTrainer(\n",
    "    model=base_model,\n",
    "    args=orpo_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d389a864-e0ed-474f-b4d3-f3dd35314fc8",
   "metadata": {},
   "source": [
    "In the above code snippet, base_model is the model we are going to fine-tune, orpo_args defined before, we use \"train split\" of our DPO-compatible dataset for model fine-tuning. We can also apply peft techniques to reduce the training time. Finally perform the fine-tuning!"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
