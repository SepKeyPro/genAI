{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1c6db12-454c-4ad4-8d39-162dbbb751cf",
   "metadata": {},
   "source": [
    "# Write your first Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eba597b-1b08-4ee5-a483-921d3e99c025",
   "metadata": {},
   "source": [
    "Before transformers, recurrent neural networks (RNNs) were considered the cutting edge in Natural Language Processing (NLP). An RNN is a type of neural network where outputs from previous steps are fed as inputs to the current step. This characteristic enables an RNN to retain information from previous steps, making them well-suited for sequential data like text. In the context of NLP, an RNN takes an input, such as a word or character, processes it through its network, and generates a vector known as the hidden state. If you are unfamiliar with RNNs, don't worry, you don't need to know the detailed workings of RNNs to follow this discussion. \n",
    "\n",
    "One area where RNNs played an important role was in the development of machine translation systems, where the model translates text from one language to another. However, the word sequence in one language might be different from another one due to the grammatical structures in the source and target language. To address this issue we can use an encoder-decoder architecture. The encoder's role is to convert input sequence information into a numerical representation, typically referred to as the final hidden state. The encoder updates its hidden state at each step, trying to capture the entire meaning of the input sentence in the final hidden state. The decoder then takes this final hidden state to start generating the translated sentence, one word at a time. \\\n",
    "However, a significant challenge of this architecture lies in the fact that the final hidden state of the encoder creates an information bottleneck. it has to represent the meaning of the whole input sequence because this is all the decoder has access to when generating the output. This is especially challenging for long sequences, where information at the start of the sequence might be lost in the process of compressing everything to a single, fixed representation.\n",
    "\n",
    "To address this challenge, an \"attention mechanism\" is introduced, permitting the decoder to selectively access different hidden states of the encoder. But, why selective? Using all the states at the same time would create a huge input for the decoder, the attention mechanism lets the decoder assign a different amount of weight, or \"attention\" to each of the encoder states at every decoding timestep. \\\n",
    "Researchers, as detailed in the paper \"Attention is all you need,\" have demonstrated that RNN architectures are not required for NLP applications such as machine translation and proposed a transformer architecture with a “self-attention mechanism”.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b96e7d-82cd-4231-a8e8-5c054b3aa96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "%pip install torch==2.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff54b0f2-c383-425a-9f3a-1c2edd3ac0a5",
   "metadata": {},
   "source": [
    "The main idea behind the self-attention mechanism is that instead of using fixed embeddings for each token, we can use the whole sequence to compute a weighted average of each embedding. Given a sequence of token embeddings $ x_{1}, ..., x_{n} $ self-attention produces a sequence of new embeddings $ x_{1}^{'}, ..., x_{n}^{'} $ where each $ x_{i}^{'} $ is a linear combination of all the $ x_{j}^{'},  j=1...n $: \n",
    "\n",
    "$(1) \\; x_{i} = \\sum \\limits _{j=1} ^{n} w_{ji}x_{j}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88efa144-ee58-4731-9bc4-cdde2840e6f3",
   "metadata": {},
   "source": [
    "There are several ways to implement a self-attention layer. The original implementation  introduced in the paper “” is called \"Scaled Dot-Product Attention\" \n",
    "\n",
    "$ (2) \\; Attention(Q,K,V) =  softmax(\\frac{QK^{T}}{\\sqrt{d_{k}}})V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a5dc7f5-b211-40e4-abf6-fc9c2b900b89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471a2c97-efa1-41e7-9bf6-558e800ac342",
   "metadata": {},
   "source": [
    "Putting these steps together, we will have the following function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e36dfc-1c3a-4ecf-920c-601248af4060",
   "metadata": {},
   "source": [
    "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. \n",
    "\n",
    "$(3) \\; MultiHead(Q,K,V) =  Concat(head_{1}, ..., head_{h})W^{O} $ \\\n",
    "\\\n",
    "where $ head_{i} = Attention(QW_{i}^{Q},KW_{i}^{K},VW{i}^{V})$ and, \\\n",
    "$ W_{i}^{Q} \\in \\mathbb{R}^{d_{model}\\times d_{k}}, W_{i}^{K} \\in \\mathbb{R}^{d_{model} \\times d_{k}}, W_{i}^{V} \\in \\mathbb{R}^{d_{model} \\times d_{v}} and W^{O} \\in \\mathbb{R}^{d_{hv} \\times d_{model}} $ are weight matrices.\n",
    "\n",
    "These three weight matrices are used to project the embedded input tokens, x(i), into query, key, and value vectors.\n",
    "\n",
    "These matrices transform input data into queries, keys, and values, which are crucial components of the attention mechanism. As the model is exposed to more data during training, it adjusts these trainable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a703e4b-206a-44cf-a9f1-01d6a2d6c18f",
   "metadata": {},
   "source": [
    "# What are Query, Key and Value ? \n",
    "\n",
    "In attention mechanisms, we use terms like \"key,\" \"query,\" and \"value\" which come from information retrieval and databases. They help us store, search, and get information efficiently.\n",
    "\n",
    "Think of a \"query\" like a search term you put into a database. It's what the model is currently focusing on or trying to understand, like a word in a sentence. The query helps the model figure out how much attention to give to other parts of the input.\n",
    "\n",
    "A \"key\" is like an index in a database used for searching. Each item in the input sequence, such as each word in a sentence, has a key. These keys are matched with the query to find relevant information.\n",
    "\n",
    "The \"value\" in this context is similar to the value in a key-value pair in a database. It represents the actual content or representation of the input items. Once the model determines which keys (and thus which parts of the input) are most relevant to the query (the current focus item), it retrieves the corresponding values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2ca443-6607-4524-b016-b89006399574",
   "metadata": {},
   "source": [
    "With this introduction, let's code our very first attention mechanism. Imagine we have an embedding model that generates embeddings in a 5 dimentional embedding space. Assume that our embedding model has generated the following embedding vectors for our input sentence \"Write your first Attention mechanism\".  \n",
    "\n",
    "Please note that embedding values in this example are totally random and dosen't express any information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94041d69-e0b0-4532-81e3-c2f807213b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Write your very first Attention mechanism\n",
    "inputs = torch.tensor(\n",
    "  [[0.172, 0.295, 0.618, 0.459, 0.818], # Write \n",
    "   [0.265, 0.563, 0.718, 0.323, 0.126], # your  \n",
    "   [0.071, 0.235, 0.594, 0.954, 0.418], # very   \n",
    "   [0.206, 0.333, 0.044, 0.862, 0.152], # first    \n",
    "   [0.300, 0.505, 0.727, 0.495, 0.898], # Attention     \n",
    "   [0.095, 0.809, 0.596, 0.110, 0.447]] # mechanism   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd2ba4a9-a682-4660-b75f-3afbee5720e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3000, 0.5050, 0.7270, 0.4950, 0.8980])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[4]\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f256b66-a73d-473a-b6f0-b20329f3bff2",
   "metadata": {},
   "source": [
    "First we should generate attention weight matrix, which simply is the dot product of each embedding vector with other embedding vecotrs. \n",
    "\n",
    "That is $ Attention Scores \\in \\mathbb{R}^{d_{t}\\times d_{t}} $ \\\n",
    "Where $ d_{t} $ is the number of input tokens (i.e., words), here $ d_{t} = 6 $   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7603bcec-c0dc-4b35-b14e-ff273a991f56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "token_nums = inputs.shape[0]\n",
    "attn_scores = torch.zeros(token_nums, token_nums)\n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7281b94c-a683-4e27-be61-16b37a0ab9f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3783, 0.9067, 1.2284, 0.6809, 1.6116, 1.0395],\n",
      "        [0.9067, 1.0229, 0.9384, 0.5712, 1.1588, 1.0004],\n",
      "        [1.2284, 0.9384, 1.4979, 1.0049, 1.4194, 0.8427],\n",
      "        [0.6809, 0.5712, 1.0049, 0.9214, 0.8251, 0.4780],\n",
      "        [1.6116, 1.1588, 1.4194, 0.8251, 1.9250, 1.3262],\n",
      "        [1.0395, 1.0004, 0.8427, 0.4780, 1.3262, 1.2306]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.matmul(inputs, inputs.T) \n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fec6e41b-7914-4a8a-9a0c-16137a4665f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2017, 0.1599, 0.1740, 0.1533, 0.1986, 0.1696],\n",
      "        [0.1259, 0.1796, 0.1302, 0.1374, 0.1263, 0.1631],\n",
      "        [0.1736, 0.1650, 0.2278, 0.2120, 0.1638, 0.1393],\n",
      "        [0.1004, 0.1143, 0.1391, 0.1950, 0.0904, 0.0967],\n",
      "        [0.2547, 0.2057, 0.2106, 0.1771, 0.2716, 0.2259],\n",
      "        [0.1437, 0.1756, 0.1183, 0.1252, 0.1493, 0.2053]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=0)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "169d59a4-2388-410f-8542-586ec4b164ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1967, 0.4789, 0.5950, 0.5593, 0.5353],\n",
      "        [0.1602, 0.4103, 0.4791, 0.4388, 0.3872],\n",
      "        [0.1958, 0.4637, 0.5726, 0.6295, 0.4997],\n",
      "        [0.1339, 0.3155, 0.3587, 0.4392, 0.3088],\n",
      "        [0.2527, 0.6194, 0.7701, 0.6962, 0.6941],\n",
      "        [0.1697, 0.4522, 0.5215, 0.4399, 0.4340]])\n"
     ]
    }
   ],
   "source": [
    "context_vectors = torch.matmul(attn_weights, inputs)  \n",
    "print(context_vectors) # 6x5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4890497-c103-46ae-b82c-f73d8f21b819",
   "metadata": {},
   "source": [
    "As you can see, context vectors are the same size as our inputs. In other way, we simply modified the embeddings to reflect the attention to other tokens as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cf12ea-923a-407b-ad78-67ffb24fea88",
   "metadata": {},
   "source": [
    "# Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c644237b-9a7b-4f25-bcec-91700d1fb94d",
   "metadata": {},
   "source": [
    "The paper \"Attention Is All You Need\" introduces <em> Scaled Dot-Product Attention </em>. For instance, when scaling up the embedding dimension, which is typically greater than thousand for GPT-like LLMs, large dot products can result in very small gradients during backpropagation due to the softmax function applied to them. As dot products increase, the softmax function behaves more like a step function, resulting in gradients nearing zero. These small gradients can drastically slow down learning. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. \n",
    "\n",
    "<center><figure><img src=\"imgs/scaled-dot-product.png\" alt=\"drawing\" width=\"300\"/><figcaption>Fig. 1: Scaled Dot-Product Attention.</figcaption></figure></center>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "671a0a1e-c96f-46f6-9d59-361998907582",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    print(\"key \", K.shape, \"value \",K.size)\n",
    "    dim_k = K.size(-1)\n",
    "    attn_scores = torch.matmul(Q, K.T)\n",
    "    attn_weights = torch.softmax(attn_scores / sqrt(dim_k),dim=-1)\n",
    "    return torch.matmul(attn_weights,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8e1fbc10-a6d8-4576-8a53-0a5d7344caa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "        self.W_k = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "        self.W_v = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys = self.W_k(x)\n",
    "        queries = self.W_q(x)\n",
    "        values = self.W_v(x)\n",
    "        \n",
    "        # attn_scores = queries @ keys.T\n",
    "        # attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5,dim=1)\n",
    "        # context_vec = attn_weights @ values \n",
    "        # return context_vec\n",
    "        \n",
    "        attention_outputs = scaled_dot_product_attention_simple(queries, keys, values)\n",
    "        return attention_outputs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b6538be4-7a11-4264-8132-eb94d632aaf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5500, 0.8700, 0.6600])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2366, -0.3615],\n",
       "        [ 0.2361, -0.3613],\n",
       "        [ 0.2361, -0.3613],\n",
       "        [ 0.2359, -0.3606],\n",
       "        [ 0.2353, -0.3596],\n",
       "        [ 0.2363, -0.3613]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = 3\n",
    "head_dim = 2\n",
    "\n",
    "self_attention = SelfAttention(embed_dim,head_dim)\n",
    "print(query)\n",
    "self_attention(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70d119e-df46-4856-9fa5-3b765ae41b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention_v2(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(embed_dim, num_heads, head_dim)\n",
    "        self.heads = nn.ModuleList([SelfAttention(embed_dim, head_dim) for _ in range(num_heads)])\n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, hidden_state):\n",
    "        x = torch.cat([head(hidden_state) for head in self.heads], dim=-1)\n",
    "        x = self.output_linear(x)\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4595f4c7-7a73-4ab5-854e-b827bfc47ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
