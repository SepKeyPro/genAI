{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e36c551b-594b-4233-85d6-0af199b5df99",
   "metadata": {},
   "source": [
    "# Corrective Retrieval Augmented Generation (CRAG)\n",
    "\n",
    "Corrective-RAG (CRAG) is a recent paper that introduces an interesting approach for self-reflective RAG. You can read the paper [here](https://arxiv.org/pdf/2401.15884.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6d0f13-5264-46c7-a4a3-e606adbd00ca",
   "metadata": {},
   "source": [
    "retrieval augmented generation (RAG) has introduced a retrieval technique to incorporate relevant knowledge to the model's input therefore improving output generation. Within this framework, models receive augmented input by adding relevant documents retrieved from an external knowledge collections. \n",
    "\n",
    "While RAG acts as a viable complement to LLMs, its efficiency relies heavily on the relevance and accuracy of the retrieved documents. The substantial dependence of generation on the retrieved knowledge raises notable concerns regarding the model's behavior and performance in scenarios where retrieval is not successful or the retrieved documents are inaccurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6222fa-3481-4790-8fc4-764412f9898e",
   "metadata": {},
   "source": [
    "A low-quality retriever can bring in a lot of irrelevant information. This can make it hard for models to acquire accurate knowledge and might even mislead them, causing problems like hallucinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e21d8a-6f1d-4f28-8ecd-374be75ef49d",
   "metadata": {},
   "source": [
    "Figure 1 shows how CRAG works at inference, in order to make generation more resilient. Given an input query and the retrieved documents from a retriever, CRAG uses a lightweight evaluator to estimate the relevance score of retrieved documents to the input query. This evaluation results in three confidence degrees and then triggered the corresponding actions: {Correct, Incorrect, Ambiguous}. If it's Correct, the retrieved documents are improved to be more accurate through knowledge refienment processes. This refinement operation involves knowledge decomposition, filter, and recomposition. If it's Incorrect, the retrieved documents are ignored, and web searches are used instead as complementary knowledge sources for corrections. If it's not clear whether the documents are correct or not, an action called Ambiguous is taken, combining both. Once the retrieval is refined, any generative model can be used.\n",
    "\n",
    "<center><figure><img src=\"imgs/CRAG.jpg\" alt=\"drawing\" width=\"700\"/><figcaption>Fig. 1: An overview of CRAG at inference.</figcaption></figure></center>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60c6d93-af3f-442b-82c6-0197250213f5",
   "metadata": {},
   "source": [
    "## Knowledge Refinement \n",
    "A retrieval is considered Correct if the confidence score of at least one retrieved document exceeds the upper threshold. This means the presence of relevant documents in the retrieval results. However, even when a relevant document is found, it may contain some irrelevant information. To extract the most important information within this document, a method called knowledge refinement is applied. This method involves decomposing and then recomposing the content of each retrieved relevant document to extract the most crucial information. Initially, each document is divided into smaller knowledge segments through heuristic rules. Then, a fine-tuned retrieval evaluator assesses the relevance score of each segment. Based on these scores, irrelevant segments are filtered out, and relevant ones are recomposed via concatenation in order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ba8819-3d46-4be2-bac4-163c9842d993",
   "metadata": {},
   "source": [
    "# CRAG implementation in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4851944-e664-4c37-b252-c627f71fd655",
   "metadata": {},
   "source": [
    "We can use LangGraph of LangChain to implement CRAG. So, first let's see what [LangGraph](https://python.langchain.com/docs/langgraph) is .\n",
    "\n",
    "## LangGraph\n",
    "LLMs can be used for reasoning tasks. This can essentially be thought of as running an LLM in a for-loop. These types of systems are often called agents. Here comes LangGraph! You may want to always force an agent to call a particular tool first. You may want to have more control over agents and how tools are called. These more controlled flows are referred to as \"state machines\" in LangGraph terminology. LangGraph is a way to create these state machines by specifying them as graphs.\n",
    "\n",
    "The primary function of LangGraph is to add cycles into LLM applications. Cycles play a vital role in scenarios with agents. For example, you might repeatedly invoke an LLM within a loop to determine the next course of action.\n",
    "LangGraph is a tool designed for creating complex, stateful applications that involve multiple actors using LLMs. It is built upon LangChain and expands its capabilities by enabling the coordination of multiple chains or actors through various steps of computation in a cyclic fashion. \n",
    "It's important to note that LangGraph is not a **Directed Acyclic Graph (DAG)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de1d270-628f-43dc-b74d-3b9dfca45921",
   "metadata": {},
   "source": [
    "### How to build a graph using LangGraph\n",
    "\n",
    "#### Create SateGraph\n",
    "Graphs in langgraph are the `StatefulGraph`. This graph is parameterized by a state object that it passes around to each node. This state definition represents a central state object that is updated over time by each node. These operations can either `set` specific attributes on the state (e.g. overwrite the existing values) or `add` to the existing attribute in the form of a key-value store. \n",
    "#### Add Nodes\n",
    "We will add nodes to the graph with `(name, value)` pair, where name is node's name that will be used to refer to the node when adding edges. The value is a function or LCEL runnable that will be called.\n",
    "#### Add Edges\n",
    "After adding nodes, we can then add edges to the graph. We have two types of edges.\n",
    "* ##### Normal Edges\n",
    "These are edges where one node should ALWAYS be called after another.\n",
    "* ##### Conditional Edges\n",
    "These are edges where based on the output of a node, one of several paths is taken. After the agent is called: if the agent said to take an action, then the function to invoke tools should be called, or if the agent said that it was finished, then it should finish.\n",
    "#### Compile it!\n",
    "Finally we can compile it into a runnable! This simply takes the graph definition and returns a runnable. This runnable exposes all the same method as LangChain runnables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66b7430-88d3-4807-a822-02dacfec1d27",
   "metadata": {},
   "source": [
    "# CRAG example \n",
    "Lets code a simple CRAG in LangGraph! \\\n",
    "This example has two confidence score Correct and Incorrect and knowledge refinement process is eliminated. A pdf file about the United States is used as our knowledge base. First, I split the document into chunks and store their embeddings in a vector database. Then, I retrieve the relevant splits to the input question. Using a prompt, I ask the underlying LLM to assess each chunk more carefully and evaluate it with YES, or NO based on the relevance to the input question. If there is a chuck that LLM marked as NO, we perform a web search to yield a better output. Otherwise (i.e., all chucks are marked as YES), we only use the retrieved documents to render the final output. Finally, with or without web knowledge, we go through a classic RAG process to ask LLM generate the final output. Our graph consists of the following nodes and edges (see figure 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbc453f-aad7-4c20-a0c0-87be5d890820",
   "metadata": {},
   "source": [
    "### Graph nodes\n",
    "Our graph consists of following nodes:\n",
    "* ### retrive:\n",
    "This node is responsible for retrieving relevant documents to the input question\n",
    "* ### evaluate document:\n",
    "This node evaluates each retrived document by `retrieve` node. If not all documents are evaluated as `relevant`, the node will turn `web_search` flag to `True` to run a web search on the topic by the next node (i.e., `web_search` node). It also updates documents attribute to only include relevant documents.\n",
    "* ### generate knowledge keywords:\n",
    "If search flag is on, this node will generate knowledge keywords from the question as web search queries.\n",
    "* ### generate: \n",
    "This node will generate the answer. Whether this is an internal answer retrieved from the provided documents or is an external answer collected from web search. This node perform a classic RAG algorithm on the question with the provided context. \n",
    "* ### web search\n",
    "After generating the keywords from the question by `generate_knowledge_keywords` node, this node will perform a web search to find relevant documents to the input question. \n",
    "\n",
    "#### Each node receives graph sate as an argument and adds or updates its attributes upon returning it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8d411b-3de4-49bb-a4aa-04bdc7e8ca79",
   "metadata": {},
   "source": [
    "### Graph edges\n",
    "#### Normal edges:\n",
    "* (retrieve,evaluate_documents)\n",
    "* (evaluate_documents, should_generate)\n",
    "* (generate_knowledge_keywords, web_search)\n",
    "* (web_search, generate)\n",
    "\n",
    "#### Conditional edges:\n",
    "##### should_generate: Based on the search flag, it decides to generate the final answer (i.e., goes to `generate` node if flag is on) or geneate keywords for the web search (i.e., goes to `generate_knowledge_keywords` node if flag in off).\n",
    "* (should_generate, web_search)\n",
    "* (should_generate, generate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8654b32-0a47-4297-88cd-341e3367008e",
   "metadata": {},
   "source": [
    "<center><figure><img src=\"imgs/crag_langGraph.jpg\" alt=\"drawing\" width=\"900\"/><figcaption>Fig. 2: example's graph.</figcaption></figure></center>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27913437-942b-4a0d-b293-8ad03ab267fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install langchain_community faiss-cpu tiktoken langchain-openai langchainhub langchain langgraph tavily-python pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0042fa1d-51b3-43d8-b8b2-20c66679c9d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your OpenAI API Key:  ········\n",
      "Enter your Tavily API Key:  ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "os.environ['OPENAI_API_KEY'] = getpass('Enter your OpenAI API Key: ')\n",
    "os.environ['TAVILY_API_KEY'] = getpass('Enter your Tavily API Key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6975a9e5-3d17-4da9-bed7-f0572f78b3aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=10\n",
    ")\n",
    "chunks = text_splitter.split_documents(PyPDFLoader(\"docs/the-usa.pdf\").load())\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bfda5ef-023f-4aae-a924-5d7a5d54de64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.output_parsers.openai_tools import JsonOutputToolsParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import BaseMessage, FunctionMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "\n",
    "    print(\"Retrieving documents---\")\n",
    "    question = state[\"attribs\"][\"question\"]\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    return {\"attribs\": {\"documents\": documents, \"question\": question}}\n",
    "\n",
    "\n",
    "def evaluate_documents(state):\n",
    "    \n",
    "    print(\"Evaluating documents------\")\n",
    "    question = state[\"attribs\"][\"question\"]\n",
    "    documents = state[\"attribs\"][\"documents\"]   \n",
    "\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"evaluate\",\n",
    "                \"description\": \"Predict the relevance score for each question-document pair\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"score\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Score could be yes or no\",\n",
    "                        },                       \n",
    "                    },\n",
    "                    \"required\": [\"score\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"Given a question, does the following document have exact information to answer the question?\n",
    "                    Question: {question}\n",
    "                    Document: {context}\n",
    "                    Think Step by step, and answer with yes or no only\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\", streaming=True)\n",
    "    llm_with_tool = llm.bind(\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\"\n",
    "    )\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool | JsonOutputToolsParser()\n",
    "\n",
    "\n",
    "    relevant_docs = []\n",
    "    for d in documents:\n",
    "        eval_result = chain.invoke({\"question\": question, \"context\": d.page_content})        \n",
    "        score = eval_result[0]['args']['score']\n",
    "        print(\"score\",score)\n",
    "        if score == \"yes\":\n",
    "            print(\"---Evaluation Result: DOCUMENT RELEVANT---\")\n",
    "            relevant_docs.append(d)\n",
    "        else:\n",
    "            print(\"---Evaluation Result: DOCUMENT IRRELEVANT---\")\n",
    "    search = \"No\"  if len(relevant_docs) else \"Yes\" # Perform web search\n",
    "\n",
    "\n",
    "    return {\"attribs\": { \"documents\": relevant_docs, \"question\": question, \"run_web_search\": search}}\n",
    "\n",
    "\n",
    "def generate_knowledge_keywords(state):\n",
    "    \n",
    "    print(\"Generating knowledge keywords from the question for web search-----\")\n",
    "    question = state[\"attribs\"][\"question\"]\n",
    "    documents = state[\"attribs\"][\"documents\"]\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"Extract at most three keywords separated by comma from the following dialogues and questions as queries for the\n",
    "                    web search, including topic background within dialogues and main intent within questions.\n",
    "                    question: What is Henry Feilden’s occupation?\n",
    "                    query: Henry Feilden, occupation\n",
    "                    question: In what city was Billy Carlson born?\n",
    "                    query: city, Billy Carlson, born\n",
    "                    question: What is the religion of John Gwynn?\n",
    "                    query: religion of John Gwynn\n",
    "                    question: What sport does Kiribati men’s national basketball team play?\n",
    "                    query: sport, Kiribati men’s national basketball team play\n",
    "                    question: {question}\n",
    "                    query:\"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\", streaming=True)\n",
    "\n",
    "    # Prompt\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    knowledge_keywords = chain.invoke({\"question\": question})\n",
    "    \n",
    "    print(\"--- knowledge keywords-----\")\n",
    "    print(knowledge_keywords)\n",
    "\n",
    "    return {\"attribs\": {\"documents\": documents, \"question\": knowledge_keywords}}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \n",
    "    print(\"Generating output---\")\n",
    "    question = state[\"attribs\"][\"question\"]\n",
    "    documents = state[\"attribs\"][\"documents\"]\n",
    "\n",
    "    # Prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, streaming=True)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    \n",
    "    return {\n",
    "        \"attribs\": {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "    }\n",
    "\n",
    "def should_generate(state):\n",
    "    \n",
    "    print(\"Deciding on next node---\")\n",
    "    search = state[\"attribs\"][\"run_web_search\"]\n",
    "\n",
    "    if search == \"Yes\":\n",
    "        print(\"DECISION: Generate knowledge keywords and run web search---\")\n",
    "        return \"generate_knowledge_keywords\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"DECISION: Generate final output\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    \n",
    "    print(\"Running web search---\")\n",
    "    question = state[\"attribs\"][\"question\"]\n",
    "    documents = state[\"attribs\"][\"documents\"]\n",
    "\n",
    "    tool = TavilySearchResults()\n",
    "    docs = tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"attribs\": {\"documents\": documents, \"question\": question}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea86bf56-0d5b-4ace-bf0e-86be9d7c0cbf",
   "metadata": {},
   "source": [
    "## Defining a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84226e80-352d-4e90-8e7c-0332cf5350dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, TypedDict\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "class State(TypedDict):\n",
    "    attribs: Dict[str, any]\n",
    "    \n",
    "graph = StateGraph(State)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497130b0-232a-46e3-9bc7-3e47bfb91bf6",
   "metadata": {},
   "source": [
    "## Adding nodes, edges and building the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f7f53f6-f665-4815-a256-7f83ef50614f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "graph.add_node(\"evaluate_documents\", evaluate_documents)  # evaluate documents\n",
    "graph.add_node(\"generate\", generate)  # generatae\n",
    "graph.add_node(\"generate_knowledge_keywords\", generate_knowledge_keywords)  # generate_knowledge_keywords\n",
    "graph.add_node(\"web_search\", web_search)  # web search\n",
    "\n",
    "graph.set_entry_point(\"retrieve\")\n",
    "graph.add_edge(\"retrieve\", \"evaluate_documents\")\n",
    "graph.add_conditional_edges(\n",
    "    \"evaluate_documents\",\n",
    "    should_generate,\n",
    "    {\n",
    "        \"generate_knowledge_keywords\": \"generate_knowledge_keywords\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "graph.add_edge(\"generate_knowledge_keywords\", \"web_search\")\n",
    "graph.add_edge(\"web_search\", \"generate\")\n",
    "graph.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27c5f041-8965-4d17-956a-bb6ed55363c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents---\n",
      "Evaluating documents------\n",
      "score no\n",
      "---Evaluation Result: DOCUMENT IRRELEVANT---\n",
      "score no\n",
      "---Evaluation Result: DOCUMENT IRRELEVANT---\n",
      "score no\n",
      "---Evaluation Result: DOCUMENT IRRELEVANT---\n",
      "score no\n",
      "---Evaluation Result: DOCUMENT IRRELEVANT---\n",
      "Deciding on next node---\n",
      "DECISION: Generate knowledge keywords and run web search---\n",
      "Generating knowledge keywords from the question for web search-----\n",
      "--- knowledge keywords-----\n",
      "Seattle, Tell me more\n",
      "Running web search---\n",
      "Generating output---\n",
      "Final Output--------------------------------------------------------------------------------\n",
      "Tell me more about Seattle : Seattle, also known as the \"Emerald City,\" is a major port of entry and an air and sea gateway to Asia and Alaska. It is a hub for high technology and Internet-based commerce, with significant economic importance. The city is closely connected to its downtown area and has a rich history of growth and development.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"attribs\": {\"question\": \"Tell me more about Seattle\"}}\n",
    "gen = []\n",
    "for output in app.stream(inputs):  \n",
    "    gen.append(output)\n",
    "\n",
    "print(\"Final Output--------------------------------------------------------------------------------\")\n",
    "print(f\"{gen[0]['retrieve']['attribs']['question']} : {gen[-1]['__end__']['attribs']['generation']}\")\n",
    "# Final generation\n",
    "# pprint.pprint(state[\"attribs\"][\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e903c16-380b-4a44-b297-53e0e42a5aea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
