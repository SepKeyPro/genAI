{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e36c551b-594b-4233-85d6-0af199b5df99",
   "metadata": {},
   "source": [
    "# Corrective Retrieval Augmented Generation (CRAG)\n",
    "\n",
    "Corrective-RAG (CRAG) is a recent paper that introduces an interesting approach for self-reflective RAG. You can read the paper [here](https://arxiv.org/pdf/2401.15884.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6d0f13-5264-46c7-a4a3-e606adbd00ca",
   "metadata": {},
   "source": [
    "retrieval augmented generation (RAG) has introduced a retrieval technique to incorporate relevant knowledge to the model's input therefore improving output generation. Within this framework, models receive augmented input by adding relevant documents retrieved from an external knowledge collections. While RAG serves as a practicable complement to LLMs, its effectiveness is contingent upon the relevance and accuracy of the retrieved documents. The heavy reliance of generation on the retrieved knowledge raises significant concerns about the modelâ€™s behavior and performance in scenarios where retrieval may fail or return inaccurate results.\n",
    "\n",
    "While RAG acts as a viable supplement to LLMs, its efficiency relies heavily on the relevance and accuracy of the retrieved documents. The substantial dependence of generation on the retrieved knowledge raises notable concerns regarding the model's behavior and performance in scenarios where retrieval is not successful or the retrieved documents are inaccurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6222fa-3481-4790-8fc4-764412f9898e",
   "metadata": {},
   "source": [
    "A low-quality retriever can bring in a lot of irrelevant information. This can make it hard for models to acquire accurate knowledge and might even mislead them, causing problems like hallucinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e21d8a-6f1d-4f28-8ecd-374be75ef49d",
   "metadata": {},
   "source": [
    "Figure 1 shows how CRAG works at inference, in order to make generation more resilient. Given an input query and the retrieved documents from a retriever, CRAG uses a lightweight evaluator to estimate the relevance score of retrieved documents to the input query. This evaluation results in three confidence degrees and then triggered the corresponding actions: {Correct, Incorrect, Ambiguous}. If it's Correct, the retrieved documents are improved to be more accurate through knowledge refienment processes. This refinement operation involves knowledge decomposition, filter, and recomposition. If it's Incorrect, the retrieved documents are ignored, and web searches are used instead as complementary knowledge sources for corrections. If it's not clear whether the documents are correct or not, an action called Ambiguous is taken, combining both (Section 4.3). Once the retrieval is refined, any generative model can be used.\n",
    "\n",
    "<center><figure><img src=\"imgs/CRAG.jpg\" alt=\"drawing\" width=\"700\"/><figcaption>Fig. 1: An overview of CRAG at inference.</figcaption></figure></center>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60c6d93-af3f-442b-82c6-0197250213f5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
